{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Image](./snapshot_strategy.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1차에서 수행한 100개의 질문을 제외하고 다시 랜덤하게 100개 추출\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "file_pre = '4th'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "p = os.path.abspath('..')\n",
    "pp = os.path.abspath('../..')\n",
    "# p = p+r'\\config'\n",
    "sys.path.insert(1, p)\n",
    "sys.path.insert(1, pp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-26 14:16:45 [__init__.py:216] Automatically detected platform cuda.\n"
     ]
    }
   ],
   "source": [
    "import psycopg2\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import config.config as conf\n",
    "import pickle\n",
    "import lib.preprocess.preprocess as pp\n",
    "import lib.preprocess.SectionExtractor as se\n",
    "import lib.annotation.D_Annotation as da\n",
    "import lib.annotation.Self_Consistency as sc\n",
    "import re\n",
    "import datetime\n",
    "import pandas as pd\n",
    "import re\n",
    "import numpy as np\n",
    "from sklearn import metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "htmlp = pp.HTMLParser()\n",
    "codep = pp.CodeSectionParser()\n",
    "ts = se.SectionExtractor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_sql_2223 = \"\"\"select a.id \n",
    "                , a.creationdate\n",
    "                , a.title\n",
    "                , b.body\n",
    "            from posts a \n",
    "               , postsbody b\n",
    "            where a.id = b.id\n",
    "              and a.creationdate between '2021-11-30' and '2023-12-01' \n",
    "              and a.posttypeid = '1'\n",
    "              and a.tags like '%<python>%' \n",
    "              and not exists(select 1 \n",
    "                                    from public.tt_posts_difficulty_annotated x \n",
    "                                where a.id = x.id)\n",
    "              and not exists(select 1 \n",
    "                                    from tt_posts_difficulty_done xx \n",
    "                                where a.id = xx.id)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2324의 경우 public_for_2324 스키마 tt_posts_difficulty_annotated테이블을 보고 제외\n",
    "q_sql_2324 = \"\"\"select a.id \n",
    "                , a.creationdate\n",
    "                , a.title\n",
    "                , b.body\n",
    "            from public_for_2324.posts a \n",
    "               , public_for_2324.postsbody b\n",
    "            where a.id = b.id\n",
    "              and a.creationdate between '2021-11-30' and '2024-12-03' \n",
    "              and a.posttypeid = '1'\n",
    "              and a.tags like '%<python>%' \n",
    "              and not exists(select 1 \n",
    "                                    from public_for_2324.tt_posts_difficulty_annotated x \n",
    "                                where a.id = x.id)\n",
    "              and not exists(select 1 \n",
    "                                    from tt_posts_difficulty_done xx \n",
    "                                where a.id = xx.id)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = psycopg2.connect(host = conf.database_user['host'], dbname=conf.database_user['dbname'], user=conf.database_user['user'], password=conf.database_user['password'])\n",
    "try:\n",
    "    cur = conn.cursor()\n",
    "    cur.execute(q_sql_2223)\n",
    "    rows = cur.fetchall()\n",
    "    \n",
    "\n",
    "except psycopg2.DatabaseError as db_err:\n",
    "    print(db_err)\n",
    "finally : \n",
    "  cur.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_output_for_2223 = pd.DataFrame(rows, columns = [\n",
    "  'id'\n",
    "  ,'creationdate'\n",
    "  ,'title'\n",
    "  , 'body'\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_output_for_2223['22_yn'] = np.where(pd.to_datetime(q_output_for_2223['creationdate']).dt.date < datetime.date(2022, 12, 1), 1, 0)\n",
    "\n",
    "q_output_for_2223_22 = q_output_for_2223.loc[q_output_for_2223['22_yn'] == 1, :]\n",
    "q_output_for_2223_23 = q_output_for_2223.loc[q_output_for_2223['22_yn'] == 0, :]\n",
    "# first_ann_q_id = np.random.choice(list(q_output['id']), size=30, replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qid_for_2223_22 = np.random.choice(list(q_output_for_2223_22['id']), size=54, replace=False)\n",
    "qid_for_2223_23 = np.random.choice(list(q_output_for_2223_23['id']), size=54, replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conn = psycopg2.connect(host = conf.database_user['host'], dbname=conf.database_user['dbname'], user=conf.database_user['user'], password=conf.database_user['password'])\n",
    "try:\n",
    "    cur = conn.cursor()\n",
    "    cur.execute(q_sql_2324)\n",
    "    rows = cur.fetchall()\n",
    "    \n",
    "\n",
    "except psycopg2.DatabaseError as db_err:\n",
    "    print(db_err)\n",
    "finally : \n",
    "  cur.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_output_for_2324 = pd.DataFrame(rows, columns = [\n",
    "  'id'\n",
    "  ,'creationdate'\n",
    "  ,'title'\n",
    "  , 'body'\n",
    "])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_output_for_2324['22_yn'] = np.where(pd.to_datetime(q_output_for_2324['creationdate']).dt.date < datetime.date(2022, 12, 1), 1, 0)\n",
    "q_output_for_2324['23_yn'] = np.where((pd.to_datetime(q_output_for_2324['creationdate']).dt.date < datetime.date(2023, 12, 2)) & (q_output_for_2324['22_yn'] ==0), 1, 0)\n",
    "q_output_for_2324['24_yn'] = np.where(pd.to_datetime(q_output_for_2324['creationdate']).dt.date >= datetime.date(2023, 12, 2), 1, 0)\n",
    "\n",
    "print(q_output_for_2324.loc[q_output_for_2324['22_yn'] ==1, 'creationdate'].min())\n",
    "print(q_output_for_2324.loc[q_output_for_2324['22_yn'] ==1, 'creationdate'].max())\n",
    "\n",
    "print(q_output_for_2324.loc[q_output_for_2324['23_yn'] ==1, 'creationdate'].min())\n",
    "print(q_output_for_2324.loc[q_output_for_2324['23_yn'] ==1, 'creationdate'].max())\n",
    "\n",
    "print(q_output_for_2324.loc[q_output_for_2324['24_yn'] ==1, 'creationdate'].min())\n",
    "print(q_output_for_2324.loc[q_output_for_2324['24_yn'] ==1, 'creationdate'].max())\n",
    "\n",
    "# q_output_for_2223_22 = q_output_for_2223.loc[q_output_for_2223['22_yn'] == 1, :]\n",
    "# q_output_for_2223_23 = q_output_for_2223.loc[q_output_for_2223['22_yn'] == 0, :]\n",
    "# first_ann_q_id = np.random.choice(list(q_output['id']), size=30, replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q_output_for_2324_22 = q_output_for_2324.loc[q_output_for_2324['22_yn'] == 1, :]\n",
    "q_output_for_2324_23 = q_output_for_2324.loc[q_output_for_2324['23_yn'] == 1, :]\n",
    "q_output_for_2324_24 = q_output_for_2324.loc[q_output_for_2324['24_yn'] == 1, :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(q_output_for_2324_22['creationdate'].min())\n",
    "print(q_output_for_2324_22['creationdate'].max())\n",
    "\n",
    "print(q_output_for_2324_23['creationdate'].min())\n",
    "print(q_output_for_2324_23['creationdate'].max())\n",
    "\n",
    "print(q_output_for_2324_24['creationdate'].min())\n",
    "print(q_output_for_2324_24['creationdate'].max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "qid_for_2324_22 = np.random.choice(list(q_output_for_2324_22['id']), size=36, replace=False)\n",
    "qid_for_2324_23 = np.random.choice(list(q_output_for_2324_23['id']), size=36, replace=False)\n",
    "qid_for_2324_24 = np.random.choice(list(q_output_for_2324_24['id']), size=36, replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chg_tag(code):\n",
    "    st_pattern = r'<pre(?: class=\"[^\"]*\")?><code>'\n",
    "    st_dst = \"```python\\n\"\n",
    "    code = re.sub(st_pattern, st_dst, code, count=0, flags=0)\n",
    "    \n",
    "    end_dst = \"```\"\n",
    "    end_pattern =r'</code></pre>'\n",
    "    code = re.sub(end_pattern, end_dst, code, count=0, flags=0)\n",
    "    return code\n",
    "                                                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_code_df(q_output, q_list):\n",
    "    q_output_copy = q_output[q_output['id'].isin(q_list)].copy()\n",
    "    q_output_copy['t_body'] = q_output_copy['body'].apply(chg_tag)\n",
    "    q_output_copy['clean_body'] = q_output_copy['t_body'].apply(lambda x : htmlp.get_html_cleaned_str(x))\n",
    "    q_output_copy['clean_body'] = q_output_copy['clean_body'].apply(lambda x:  re.sub(r\";(?=\\S)\", \"\", x))\n",
    "    q_output_copy['question'] = \"\"\"<Title>\"\"\"+q_output_copy['title'].map(str)+\"\"\"</Title>. <Question>\"\"\"+q_output_copy['clean_body'].map(str)+\"\"\"</Question> Let's think through the difficulty of question carefully, step by step. \"\"\"\n",
    "    return q_output_copy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pp_2324_22 = clean_code_df(q_output_for_2223_22, qid_for_2223_22)\n",
    "pp_2324_22 = clean_code_df(q_output_for_2223_23, qid_for_2223_23)\n",
    "\n",
    "pp_2324_22 = clean_code_df(q_output_for_2324_22, qid_for_2324_22)\n",
    "pp_2324_23 = clean_code_df(q_output_for_2324_23, qid_for_2324_23)\n",
    "pp_2324_24 = clean_code_df(q_output_for_2324_24, qid_for_2324_24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snapshop1_sample = pd.concat((pp_2324_22[['id', 'question']], pp_2324_22[['id', 'question']]))\n",
    "snapshop2_sample = pd.concat((pp_2324_22[['id', 'question']], pp_2324_23[['id', 'question']], pp_2324_24[['id', 'question']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snapshop1_sample[['id', 'question']].to_csv(f'./{file_pre}_snapshop1_sample.csv')\n",
    "snapshop2_sample[['id', 'question']].to_csv(f'./{file_pre}_snapshop2_sample.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## public_for_2324.tt_posts_difficulty_annotated, public.tt_posts_difficulty_annotated 테이블에 인서트 필요"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list(snapshop1_sample['id'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scn_smp = pd.read_csv(f'./2nd_snapshop2_sample.csv')\n",
    "thd_smp = pd.read_csv(f'./3rd_snapshop2_sample.csv')\n",
    "tar_smp = pd.read_csv(f'/usr/share/d_ollama/data/q_output_code_y_74_bak.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_smp = pd.concat([scn_smp, thd_smp], axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_smp.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot_df = pd.merge(tar_smp, sample_smp[['id', 'question']], on = 'id', how='left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping = {'Basic': 'Difficulty Level : Basic', \n",
    "           'Intermediate': 'Difficulty Level : Intermediate', \n",
    "           'Advanced' : 'Difficulty Level : Advanced'}\n",
    "tot_df['answer'] = tot_df['answer'].map(mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tot_df.to_csv('/usr/share/d_ollama/data/q_output_code_y_74.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task v_4_5_Y_sys_prompt10_5 시작\n",
      "Task v|4|5|Y|20|sys_prompt10|5|0.01|ver6 실행 중: 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./result/sc_v_result_4_5_Y_20_sys_prompt10_5_0.01_ver6_0.csv\n",
      "/usr/share/d_ollama/data/q_output_code_y_74.csv\n",
      "VLLM\n",
      "INFO 11-26 14:16:59 [utils.py:328] non-default args: {'tensor_parallel_size': 4, 'gpu_memory_utilization': 0.3, 'disable_log_stats': True, 'model': '/usr/share/d_ollama/.ollama/models/hf_model/Llama-3.2-3B-Instruct'}\n",
      "INFO 11-26 14:17:06 [__init__.py:742] Resolved architecture: LlamaForCausalLM\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`torch_dtype` is deprecated! Use `dtype` instead!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-26 14:17:06 [__init__.py:1815] Using max model len 131072\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-11-26 14:17:07,158\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO 11-26 14:17:07 [scheduler.py:222] Chunked prefill is enabled with max_num_batched_tokens=8192.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2500140)\u001b[0;0m INFO 11-26 14:17:08 [core.py:654] Waiting for init message from front-end.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2500140)\u001b[0;0m INFO 11-26 14:17:08 [core.py:76] Initializing a V1 LLM engine (v0.10.2) with config: model='/usr/share/d_ollama/.ollama/models/hf_model/Llama-3.2-3B-Instruct', speculative_config=None, tokenizer='/usr/share/d_ollama/.ollama/models/hf_model/Llama-3.2-3B-Instruct', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=131072, download_dir=None, load_format=auto, tensor_parallel_size=4, pipeline_parallel_size=1, data_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, device_config=cuda, decoding_config=DecodingConfig(backend='auto', disable_fallback=False, disable_any_whitespace=False, disable_additional_properties=False, reasoning_backend=''), observability_config=ObservabilityConfig(show_hidden_metrics_for_version=None, otlp_traces_endpoint=None, collect_detailed_traces=None), seed=0, served_model_name=/usr/share/d_ollama/.ollama/models/hf_model/Llama-3.2-3B-Instruct, enable_prefix_caching=True, chunked_prefill_enabled=True, use_async_output_proc=True, pooler_config=None, compilation_config={\"level\":3,\"debug_dump_path\":\"\",\"cache_dir\":\"\",\"backend\":\"\",\"custom_ops\":[],\"splitting_ops\":[\"vllm.unified_attention\",\"vllm.unified_attention_with_output\",\"vllm.mamba_mixer2\",\"vllm.mamba_mixer\",\"vllm.short_conv\",\"vllm.linear_attention\",\"vllm.plamo2_mamba_mixer\",\"vllm.gdn_attention\"],\"use_inductor\":true,\"compile_sizes\":[],\"inductor_compile_config\":{\"enable_auto_functionalized_v2\":false},\"inductor_passes\":{},\"cudagraph_mode\":1,\"use_cudagraph\":true,\"cudagraph_num_of_warmups\":1,\"cudagraph_capture_sizes\":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],\"cudagraph_copy_inputs\":false,\"full_cuda_graph\":false,\"pass_config\":{},\"max_capture_size\":512,\"local_cache_dir\":null}\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2500140)\u001b[0;0m WARNING 11-26 14:17:08 [multiproc_worker_utils.py:273] Reducing Torch parallelism from 32 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2500140)\u001b[0;0m INFO 11-26 14:17:08 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0, 1, 2, 3], buffer_handle=(4, 16777216, 10, 'psm_42b94c0d'), local_subscribe_addr='ipc:///tmp/4ea9b571-2c56-4ff6-8d37-99aed4735bde', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2500140)\u001b[0;0m INFO 11-26 14:17:10 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_9143da2d'), local_subscribe_addr='ipc:///tmp/dec468d0-fe13-4ee6-89e9-81737c95090a', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2500140)\u001b[0;0m INFO 11-26 14:17:10 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_d4495244'), local_subscribe_addr='ipc:///tmp/a6fbfe40-5e55-4056-8d54-82ae20eb0d08', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2500140)\u001b[0;0m INFO 11-26 14:17:10 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_bb30670f'), local_subscribe_addr='ipc:///tmp/4be3ccdb-9a08-4e62-a22e-c36a0a0bebfb', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2500140)\u001b[0;0m INFO 11-26 14:17:10 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[0], buffer_handle=(1, 10485760, 10, 'psm_f2d1dcc1'), local_subscribe_addr='ipc:///tmp/8d5b9aeb-da5f-4c9f-99f3-9ddb80ee5ceb', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2500140)\u001b[0;0m \u001b[1;36m(EngineCore_DP0 pid=2500140)\u001b[0;0m INFO 11-26 14:17:11 [__init__.py:1433] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2500140)\u001b[0;0m INFO 11-26 14:17:11 [__init__.py:1433] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2500140)\u001b[0;0m INFO 11-26 14:17:11 [__init__.py:1433] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2500140)\u001b[0;0m INFO 11-26 14:17:11 [pynccl.py:70] vLLM is using nccl==2.27.3\n",
      "INFO 11-26 14:17:11 [pynccl.py:70] vLLM is using nccl==2.27.3\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2500140)\u001b[0;0m \u001b[1;36m(EngineCore_DP0 pid=2500140)\u001b[0;0m INFO 11-26 14:17:11 [pynccl.py:70] vLLM is using nccl==2.27.3\n",
      "INFO 11-26 14:17:11 [__init__.py:1433] Found nccl from library libnccl.so.2\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2500140)\u001b[0;0m INFO 11-26 14:17:11 [pynccl.py:70] vLLM is using nccl==2.27.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[W1126 14:17:11.513359303 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())\n",
      "[W1126 14:17:11.513709102 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())\n",
      "[W1126 14:17:11.527120711 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())\n",
      "[W1126 14:17:11.531422075 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2500140)\u001b[0;0m \u001b[1;36m(EngineCore_DP0 pid=2500140)\u001b[0;0m \u001b[1;36m(EngineCore_DP0 pid=2500140)\u001b[0;0m WARNING 11-26 14:17:11 [custom_all_reduce.py:144] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 11-26 14:17:11 [custom_all_reduce.py:144] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2500140)\u001b[0;0m WARNING 11-26 14:17:11 [custom_all_reduce.py:144] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 11-26 14:17:11 [custom_all_reduce.py:144] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2500140)\u001b[0;0m INFO 11-26 14:17:11 [shm_broadcast.py:289] vLLM message queue communication handle: Handle(local_reader_ranks=[1, 2, 3], buffer_handle=(3, 4194304, 6, 'psm_40337905'), local_subscribe_addr='ipc:///tmp/a9c39e0c-f11c-4256-abfd-6d4183f6c896', remote_subscribe_addr=None, remote_addr_ipv6=False)\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2500140)\u001b[0;0m \u001b[1;36m(EngineCore_DP0 pid=2500140)\u001b[0;0m \u001b[1;36m(EngineCore_DP0 pid=2500140)\u001b[0;0m \u001b[1;36m(EngineCore_DP0 pid=2500140)\u001b[0;0m INFO 11-26 14:17:12 [parallel_state.py:1165] rank 0 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 0, EP rank 0\n",
      "INFO 11-26 14:17:12 [parallel_state.py:1165] rank 2 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 2, EP rank 2\n",
      "INFO 11-26 14:17:12 [parallel_state.py:1165] rank 3 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 3, EP rank 3\n",
      "INFO 11-26 14:17:12 [parallel_state.py:1165] rank 1 in world size 4 is assigned as DP rank 0, PP rank 0, TP rank 1, EP rank 1\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2500140)\u001b[0;0m \u001b[1;36m(EngineCore_DP0 pid=2500140)\u001b[0;0m \u001b[1;36m(EngineCore_DP0 pid=2500140)\u001b[0;0m \u001b[1;36m(EngineCore_DP0 pid=2500140)\u001b[0;0m WARNING 11-26 14:17:12 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "WARNING 11-26 14:17:12 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "WARNING 11-26 14:17:12 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "WARNING 11-26 14:17:12 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2500140)\u001b[0;0m \u001b[1;36m(Worker_TP3 pid=2500168)\u001b[0;0m INFO 11-26 14:17:12 [gpu_model_runner.py:2338] Starting to load model /usr/share/d_ollama/.ollama/models/hf_model/Llama-3.2-3B-Instruct...\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2500140)\u001b[0;0m \u001b[1;36m(EngineCore_DP0 pid=2500140)\u001b[0;0m \u001b[1;36m(EngineCore_DP0 pid=2500140)\u001b[0;0m \u001b[1;36m(Worker_TP0 pid=2500162)\u001b[0;0m \u001b[1;36m(Worker_TP2 pid=2500166)\u001b[0;0m \u001b[1;36m(Worker_TP1 pid=2500164)\u001b[0;0m INFO 11-26 14:17:12 [gpu_model_runner.py:2338] Starting to load model /usr/share/d_ollama/.ollama/models/hf_model/Llama-3.2-3B-Instruct...\n",
      "INFO 11-26 14:17:12 [gpu_model_runner.py:2338] Starting to load model /usr/share/d_ollama/.ollama/models/hf_model/Llama-3.2-3B-Instruct...\n",
      "INFO 11-26 14:17:12 [gpu_model_runner.py:2338] Starting to load model /usr/share/d_ollama/.ollama/models/hf_model/Llama-3.2-3B-Instruct...\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : [Gloo] Rank 0\n",
      "0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3[Gloo] Rank \n",
      "1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2500140)\u001b[0;0m \u001b[1;36m(Worker_TP3 pid=2500168)\u001b[0;0m INFO 11-26 14:17:12 [gpu_model_runner.py:2370] Loading model from scratch...\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2500140)\u001b[0;0m \u001b[1;36m(Worker_TP0 pid=2500162)\u001b[0;0m INFO 11-26 14:17:12 [gpu_model_runner.py:2370] Loading model from scratch...\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2500140)\u001b[0;0m \u001b[1;36m(Worker_TP1 pid=2500164)\u001b[0;0m INFO 11-26 14:17:12 [gpu_model_runner.py:2370] Loading model from scratch...\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2500140)\u001b[0;0m \u001b[1;36m(Worker_TP2 pid=2500166)\u001b[0;0m INFO 11-26 14:17:12 [gpu_model_runner.py:2370] Loading model from scratch...\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2500140)\u001b[0;0m \u001b[1;36m(Worker_TP3 pid=2500168)\u001b[0;0m INFO 11-26 14:17:12 [cuda.py:362] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2500140)\u001b[0;0m \u001b[1;36m(Worker_TP0 pid=2500162)\u001b[0;0m INFO 11-26 14:17:12 [cuda.py:362] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2500140)\u001b[0;0m \u001b[1;36m(Worker_TP1 pid=2500164)\u001b[0;0m INFO 11-26 14:17:12 [cuda.py:362] Using Flash Attention backend on V1 engine.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2500140)\u001b[0;0m \u001b[1;36m(Worker_TP2 pid=2500166)\u001b[0;0m INFO 11-26 14:17:12 [cuda.py:362] Using Flash Attention backend on V1 engine.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s];0m \n",
      "Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=2500140)\u001b[0;0m \u001b[1;36m(Worker_TP3 pid=2500168)\u001b[0;0m INFO 11-26 14:17:13 [default_loader.py:268] Loading weights took 0.76 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  2.86it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  2.62it/s]\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2500140)\u001b[0;0m \u001b[1;36m(Worker_TP0 pid=2500162)\u001b[0;0m \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=2500140)\u001b[0;0m \u001b[1;36m(Worker_TP0 pid=2500162)\u001b[0;0m INFO 11-26 14:17:13 [default_loader.py:268] Loading weights took 0.80 seconds\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2500140)\u001b[0;0m \u001b[1;36m(Worker_TP1 pid=2500164)\u001b[0;0m INFO 11-26 14:17:13 [default_loader.py:268] Loading weights took 0.77 seconds\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2500140)\u001b[0;0m \u001b[1;36m(Worker_TP2 pid=2500166)\u001b[0;0m INFO 11-26 14:17:13 [default_loader.py:268] Loading weights took 0.77 seconds\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2500140)\u001b[0;0m \u001b[1;36m(Worker_TP3 pid=2500168)\u001b[0;0m INFO 11-26 14:17:14 [gpu_model_runner.py:2392] Model loading took 1.5341 GiB and 0.981894 seconds\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2500140)\u001b[0;0m \u001b[1;36m(Worker_TP0 pid=2500162)\u001b[0;0m INFO 11-26 14:17:14 [gpu_model_runner.py:2392] Model loading took 1.5341 GiB and 1.025902 seconds\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2500140)\u001b[0;0m \u001b[1;36m(Worker_TP1 pid=2500164)\u001b[0;0m INFO 11-26 14:17:14 [gpu_model_runner.py:2392] Model loading took 1.5341 GiB and 1.035742 seconds\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2500140)\u001b[0;0m \u001b[1;36m(Worker_TP2 pid=2500166)\u001b[0;0m INFO 11-26 14:17:14 [gpu_model_runner.py:2392] Model loading took 1.5341 GiB and 1.028857 seconds\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2500140)\u001b[0;0m \u001b[1;36m(Worker_TP1 pid=2500164)\u001b[0;0m INFO 11-26 14:17:20 [backends.py:539] Using cache directory: /home/mghan/.cache/vllm/torch_compile_cache/0a0841cdb8/rank_1_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2500140)\u001b[0;0m \u001b[1;36m(Worker_TP1 pid=2500164)\u001b[0;0m INFO 11-26 14:17:20 [backends.py:550] Dynamo bytecode transform time: 5.61 s\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2500140)\u001b[0;0m \u001b[1;36m(Worker_TP3 pid=2500168)\u001b[0;0m INFO 11-26 14:17:20 [backends.py:539] Using cache directory: /home/mghan/.cache/vllm/torch_compile_cache/0a0841cdb8/rank_3_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2500140)\u001b[0;0m \u001b[1;36m(Worker_TP3 pid=2500168)\u001b[0;0m INFO 11-26 14:17:20 [backends.py:550] Dynamo bytecode transform time: 5.64 s\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2500140)\u001b[0;0m \u001b[1;36m(Worker_TP0 pid=2500162)\u001b[0;0m INFO 11-26 14:17:20 [backends.py:539] Using cache directory: /home/mghan/.cache/vllm/torch_compile_cache/0a0841cdb8/rank_0_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2500140)\u001b[0;0m \u001b[1;36m(Worker_TP0 pid=2500162)\u001b[0;0m INFO 11-26 14:17:20 [backends.py:550] Dynamo bytecode transform time: 5.66 s\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2500140)\u001b[0;0m \u001b[1;36m(Worker_TP2 pid=2500166)\u001b[0;0m INFO 11-26 14:17:20 [backends.py:539] Using cache directory: /home/mghan/.cache/vllm/torch_compile_cache/0a0841cdb8/rank_2_0/backbone for vLLM's torch.compile\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2500140)\u001b[0;0m \u001b[1;36m(Worker_TP2 pid=2500166)\u001b[0;0m INFO 11-26 14:17:20 [backends.py:550] Dynamo bytecode transform time: 5.80 s\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2500140)\u001b[0;0m \u001b[1;36m(Worker_TP1 pid=2500164)\u001b[0;0m INFO 11-26 14:17:22 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 1.939 s\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2500140)\u001b[0;0m \u001b[1;36m(Worker_TP3 pid=2500168)\u001b[0;0m INFO 11-26 14:17:22 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 1.886 s\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2500140)\u001b[0;0m \u001b[1;36m(Worker_TP0 pid=2500162)\u001b[0;0m INFO 11-26 14:17:22 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 1.919 s\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2500140)\u001b[0;0m \u001b[1;36m(Worker_TP2 pid=2500166)\u001b[0;0m INFO 11-26 14:17:22 [backends.py:161] Directly load the compiled graph(s) for dynamic shape from the cache, took 1.920 s\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2500140)\u001b[0;0m \u001b[1;36m(EngineCore_DP0 pid=2500140)\u001b[0;0m \u001b[1;36m(Worker_TP3 pid=2500168)\u001b[0;0m \u001b[1;36m(EngineCore_DP0 pid=2500140)\u001b[0;0m \u001b[1;36m(Worker_TP1 pid=2500164)\u001b[0;0m INFO 11-26 14:17:23 [monitor.py:34] torch.compile takes 5.64 s in total\n",
      "\u001b[1;36m(Worker_TP0 pid=2500162)\u001b[0;0m INFO 11-26 14:17:23 [monitor.py:34] torch.compile takes 5.61 s in total\n",
      "INFO 11-26 14:17:23 [monitor.py:34] torch.compile takes 5.66 s in total\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2500140)\u001b[0;0m \u001b[1;36m(Worker_TP2 pid=2500166)\u001b[0;0m INFO 11-26 14:17:23 [monitor.py:34] torch.compile takes 5.80 s in total\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2500140)\u001b[0;0m \u001b[1;36m(Worker_TP0 pid=2500162)\u001b[0;0m INFO 11-26 14:17:25 [gpu_worker.py:298] Available KV cache memory: 4.17 GiB\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2500140)\u001b[0;0m \u001b[1;36m(Worker_TP1 pid=2500164)\u001b[0;0m INFO 11-26 14:17:25 [gpu_worker.py:298] Available KV cache memory: 4.17 GiB\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2500140)\u001b[0;0m \u001b[1;36m(Worker_TP2 pid=2500166)\u001b[0;0m INFO 11-26 14:17:25 [gpu_worker.py:298] Available KV cache memory: 4.17 GiB\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2500140)\u001b[0;0m \u001b[1;36m(Worker_TP3 pid=2500168)\u001b[0;0m INFO 11-26 14:17:25 [gpu_worker.py:298] Available KV cache memory: 4.17 GiB\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2500140)\u001b[0;0m INFO 11-26 14:17:25 [kv_cache_utils.py:864] GPU KV cache size: 156,272 tokens\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2500140)\u001b[0;0m INFO 11-26 14:17:25 [kv_cache_utils.py:868] Maximum concurrency for 131,072 tokens per request: 1.19x\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2500140)\u001b[0;0m INFO 11-26 14:17:25 [kv_cache_utils.py:864] GPU KV cache size: 156,272 tokens\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2500140)\u001b[0;0m INFO 11-26 14:17:25 [kv_cache_utils.py:868] Maximum concurrency for 131,072 tokens per request: 1.19x\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2500140)\u001b[0;0m INFO 11-26 14:17:25 [kv_cache_utils.py:864] GPU KV cache size: 156,272 tokens\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2500140)\u001b[0;0m INFO 11-26 14:17:25 [kv_cache_utils.py:868] Maximum concurrency for 131,072 tokens per request: 1.19x\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2500140)\u001b[0;0m INFO 11-26 14:17:25 [kv_cache_utils.py:864] GPU KV cache size: 156,272 tokens\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2500140)\u001b[0;0m INFO 11-26 14:17:25 [kv_cache_utils.py:868] Maximum concurrency for 131,072 tokens per request: 1.19x\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 67/67 [00:05<00:00, 12.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1;36m(EngineCore_DP0 pid=2500140)\u001b[0;0m \u001b[1;36m(Worker_TP0 pid=2500162)\u001b[0;0m INFO 11-26 14:17:32 [gpu_model_runner.py:3118] Graph capturing finished in 6 secs, took 0.91 GiB\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2500140)\u001b[0;0m \u001b[1;36m(EngineCore_DP0 pid=2500140)\u001b[0;0m \u001b[1;36m(Worker_TP0 pid=2500162)\u001b[0;0m \u001b[1;36m(Worker_TP2 pid=2500166)\u001b[0;0m INFO 11-26 14:17:32 [gpu_worker.py:391] Free memory on device (18.55/23.55 GiB) on startup. Desired GPU memory utilization is (0.3, 7.06 GiB). Actual usage is 1.53 GiB for weight, 1.22 GiB for peak activation, 0.13 GiB for non-torch memory, and 0.91 GiB for CUDAGraph memory. Replace gpu_memory_utilization config with `--kv-cache-memory=3346516172` to fit into requested memory, or `--kv-cache-memory=15684205568` to fully utilize gpu memory. Current kv cache memory in use is 4481075404 bytes.\n",
      "INFO 11-26 14:17:32 [gpu_model_runner.py:3118] Graph capturing finished in 6 secs, took 0.91 GiB\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2500140)\u001b[0;0m \u001b[1;36m(Worker_TP2 pid=2500166)\u001b[0;0m INFO 11-26 14:17:32 [gpu_worker.py:391] Free memory on device (23.31/23.55 GiB) on startup. Desired GPU memory utilization is (0.3, 7.06 GiB). Actual usage is 1.53 GiB for weight, 1.22 GiB for peak activation, 0.13 GiB for non-torch memory, and 0.91 GiB for CUDAGraph memory. Replace gpu_memory_utilization config with `--kv-cache-memory=3346516172` to fit into requested memory, or `--kv-cache-memory=20788018176` to fully utilize gpu memory. Current kv cache memory in use is 4481075404 bytes.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2500140)\u001b[0;0m \u001b[1;36m(Worker_TP1 pid=2500164)\u001b[0;0m INFO 11-26 14:17:32 [gpu_model_runner.py:3118] Graph capturing finished in 6 secs, took 0.91 GiB\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2500140)\u001b[0;0m \u001b[1;36m(Worker_TP1 pid=2500164)\u001b[0;0m \u001b[1;36m(EngineCore_DP0 pid=2500140)\u001b[0;0m INFO 11-26 14:17:32 [gpu_worker.py:391] Free memory on device (21.52/23.55 GiB) on startup. Desired GPU memory utilization is (0.3, 7.06 GiB). Actual usage is 1.53 GiB for weight, 1.22 GiB for peak activation, 0.13 GiB for non-torch memory, and 0.91 GiB for CUDAGraph memory. Replace gpu_memory_utilization config with `--kv-cache-memory=3346516172` to fit into requested memory, or `--kv-cache-memory=18863750144` to fully utilize gpu memory. Current kv cache memory in use is 4481075404 bytes.\n",
      "\u001b[1;36m(Worker_TP3 pid=2500168)\u001b[0;0m INFO 11-26 14:17:32 [gpu_model_runner.py:3118] Graph capturing finished in 6 secs, took 0.91 GiB\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2500140)\u001b[0;0m \u001b[1;36m(Worker_TP3 pid=2500168)\u001b[0;0m INFO 11-26 14:17:32 [gpu_worker.py:391] Free memory on device (23.31/23.55 GiB) on startup. Desired GPU memory utilization is (0.3, 7.06 GiB). Actual usage is 1.53 GiB for weight, 1.22 GiB for peak activation, 0.13 GiB for non-torch memory, and 0.91 GiB for CUDAGraph memory. Replace gpu_memory_utilization config with `--kv-cache-memory=3346516172` to fit into requested memory, or `--kv-cache-memory=20788018176` to fully utilize gpu memory. Current kv cache memory in use is 4481075404 bytes.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2500140)\u001b[0;0m INFO 11-26 14:17:32 [core.py:218] init engine (profile, create kv cache, warmup model) took 18.09 seconds\n",
      "INFO 11-26 14:17:33 [llm.py:295] Supported_tasks: ['generate']\n",
      "INFO 11-26 14:17:33 [__init__.py:36] No IOProcessor plugins requested by the model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 984.81it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.07s/it, est. speed input: 9049.04 toks/s, output: 9.41 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1964.55it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.79it/s, est. speed input: 9642.11 toks/s, output: 18.23 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1662.43it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.39it/s, est. speed input: 9722.57 toks/s, output: 14.06 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1781.02it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.26it/s, est. speed input: 9736.63 toks/s, output: 12.77 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1266.40it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.19it/s, est. speed input: 9828.96 toks/s, output: 12.07 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1672.37it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.34it/s, est. speed input: 9696.68 toks/s, output: 13.60 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1741.10it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.14it/s, est. speed input: 9768.40 toks/s, output: 11.53 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1711.26it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.76it/s, est. speed input: 9573.93 toks/s, output: 17.89 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1721.09it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.31it/s, est. speed input: 9690.61 toks/s, output: 13.26 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 883.01it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.46it/s, est. speed input: 9489.78 toks/s, output: 14.83 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 838.36it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.51it/s, est. speed input: 9826.83 toks/s, output: 15.31 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1923.11it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.61it/s, est. speed input: 9634.05 toks/s, output: 16.37 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1662.43it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.36it/s, est. speed input: 9622.65 toks/s, output: 13.82 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 822.25it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.43it/s, est. speed input: 9855.09 toks/s, output: 14.49 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1641.61it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.29it/s, est. speed input: 9623.34 toks/s, output: 13.05 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1677.05it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.02s/it, est. speed input: 9714.28 toks/s, output: 9.87 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1540.32it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.48it/s, est. speed input: 9945.26 toks/s, output: 15.05 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 837.35it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.32it/s, est. speed input: 9541.05 toks/s, output: 13.38 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1516.38it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.24s/it, est. speed input: 9769.10 toks/s, output: 8.12 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 936.44it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.56it/s, est. speed input: 9408.64 toks/s, output: 15.87 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 889.38it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.88it/s, est. speed input: 10048.32 toks/s, output: 19.17 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1756.41it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.61it/s, est. speed input: 9829.84 toks/s, output: 16.30 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1900.45it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.41it/s, est. speed input: 9807.86 toks/s, output: 14.36 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1779.51it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.68it/s, est. speed input: 9881.65 toks/s, output: 17.04 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1926.64it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  2.14it/s, est. speed input: 10149.29 toks/s, output: 21.82 toks/s]\n",
      "25it [00:19,  1.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task v|4|5|Y|20|sys_prompt10|5|0.01|ver6 실행 중: 1\n",
      "./result/sc_v_result_4_5_Y_20_sys_prompt10_5_0.01_ver6_1.csv\n",
      "/usr/share/d_ollama/data/q_output_code_y_74.csv\n",
      "VLLM\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2505607)\u001b[0;0m WARNING 11-26 14:17:54 [multiproc_worker_utils.py:273] Reducing Torch parallelism from 32 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "[W1126 14:17:57.904132865 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())\n",
      "[W1126 14:17:58.274375830 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())\n",
      "[W1126 14:17:58.533537405 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())\n",
      "[W1126 14:17:58.538586629 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank [Gloo] Rank 1 is connected to 23 is connected to  peer ranks. 3Expected number of connected peer ranks is :  peer ranks. 3Expected number of connected peer ranks is : 3\n",
      "\n",
      "[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank [Gloo] Rank 23 is connected to 3 is connected to  peer ranks. 3Expected number of connected peer ranks is :  peer ranks. 3Expected number of connected peer ranks is : 3\n",
      "\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2505607)\u001b[0;0m \u001b[1;36m(EngineCore_DP0 pid=2505607)\u001b[0;0m \u001b[1;36m(EngineCore_DP0 pid=2505607)\u001b[0;0m \u001b[1;36m(EngineCore_DP0 pid=2505607)\u001b[0;0m WARNING 11-26 14:17:58 [custom_all_reduce.py:144] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 11-26 14:17:58 [custom_all_reduce.py:144] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 11-26 14:17:58 [custom_all_reduce.py:144] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 11-26 14:17:58 [custom_all_reduce.py:144] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2505607)\u001b[0;0m \u001b[1;36m(EngineCore_DP0 pid=2505607)\u001b[0;0m \u001b[1;36m(EngineCore_DP0 pid=2505607)\u001b[0;0m \u001b[1;36m(EngineCore_DP0 pid=2505607)\u001b[0;0m WARNING 11-26 14:17:59 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "WARNING 11-26 14:17:59 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "WARNING 11-26 14:17:59 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "WARNING 11-26 14:17:59 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3[Gloo] Rank \n",
      "2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s];0m \n",
      "Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  2.00it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  3.15it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  2.88it/s]\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2505607)\u001b[0;0m \u001b[1;36m(Worker_TP0 pid=2505617)\u001b[0;0m \n",
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 67/67 [00:05<00:00, 13.17it/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1141.93it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.59it/s, est. speed input: 8856.07 toks/s, output: 16.07 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1745.44it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.40it/s, est. speed input: 9361.97 toks/s, output: 14.19 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1519.68it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.49it/s, est. speed input: 9415.00 toks/s, output: 15.14 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1693.98it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.11it/s, est. speed input: 9837.77 toks/s, output: 11.20 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1673.70it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.57it/s, est. speed input: 9841.61 toks/s, output: 15.89 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1865.79it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.91it/s, est. speed input: 10031.30 toks/s, output: 19.32 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 913.39it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.90it/s, est. speed input: 9693.46 toks/s, output: 19.37 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1651.30it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.18it/s, est. speed input: 10154.08 toks/s, output: 11.95 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1661.11it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.24it/s, est. speed input: 10267.21 toks/s, output: 12.55 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1879.17it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.59it/s, est. speed input: 9731.57 toks/s, output: 16.16 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1486.29it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.25it/s, est. speed input: 9733.64 toks/s, output: 12.64 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1487.87it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.87it/s, est. speed input: 9710.93 toks/s, output: 19.03 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1606.40it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.57it/s, est. speed input: 9842.40 toks/s, output: 15.86 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1744.72it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.40it/s, est. speed input: 9605.20 toks/s, output: 14.12 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1688.53it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.39it/s, est. speed input: 10130.74 toks/s, output: 14.05 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1887.63it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.56it/s, est. speed input: 9728.46 toks/s, output: 15.85 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1767.51it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.66it/s, est. speed input: 9820.96 toks/s, output: 16.82 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1685.14it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.09it/s, est. speed input: 9903.25 toks/s, output: 11.02 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1199.06it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.38s/it, est. speed input: 9750.73 toks/s, output: 7.29 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1730.32it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.46it/s, est. speed input: 10116.08 toks/s, output: 14.83 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 800.59it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.17it/s, est. speed input: 9879.29 toks/s, output: 11.86 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1777.25it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.27it/s, est. speed input: 9704.66 toks/s, output: 12.81 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1680.41it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.67it/s, est. speed input: 10443.74 toks/s, output: 16.94 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1632.02it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.62it/s, est. speed input: 9969.66 toks/s, output: 16.50 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1757.88it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.36it/s, est. speed input: 9747.45 toks/s, output: 13.73 toks/s]\n",
      "25it [00:19,  1.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task v|4|5|Y|20|sys_prompt10|5|0.01|ver6 실행 중: 2\n",
      "./result/sc_v_result_4_5_Y_20_sys_prompt10_5_0.01_ver6_2.csv\n",
      "/usr/share/d_ollama/data/q_output_code_y_74.csv\n",
      "VLLM\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2510714)\u001b[0;0m WARNING 11-26 14:18:41 [multiproc_worker_utils.py:273] Reducing Torch parallelism from 32 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "[W1126 14:18:44.259211450 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())\n",
      "[W1126 14:18:44.305439663 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())\n",
      "[W1126 14:18:44.908403338 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())\n",
      "[W1126 14:18:44.911235422 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2510714)\u001b[0;0m \u001b[1;36m(EngineCore_DP0 pid=2510714)\u001b[0;0m \u001b[1;36m(EngineCore_DP0 pid=2510714)\u001b[0;0m \u001b[1;36m(EngineCore_DP0 pid=2510714)\u001b[0;0m WARNING 11-26 14:18:45 [custom_all_reduce.py:144] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 11-26 14:18:45 [custom_all_reduce.py:144] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 11-26 14:18:45 [custom_all_reduce.py:144] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 11-26 14:18:45 [custom_all_reduce.py:144] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2510714)\u001b[0;0m WARNING 11-26 14:18:45 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2510714)\u001b[0;0m \u001b[1;36m(EngineCore_DP0 pid=2510714)\u001b[0;0m WARNING 11-26 14:18:45 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "WARNING 11-26 14:18:45 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2510714)\u001b[0;0m WARNING 11-26 14:18:45 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0[Gloo] Rank  peer ranks. Expected number of connected peer ranks is : 00 is connected to 0\n",
      " peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s];0m \n",
      "Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  2.09it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  3.27it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  3.00it/s]\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2510714)\u001b[0;0m \u001b[1;36m(Worker_TP0 pid=2510724)\u001b[0;0m \n",
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 67/67 [00:05<00:00, 13.26it/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1347.35it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.56it/s, est. speed input: 8923.90 toks/s, output: 15.84 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1959.04it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.75it/s, est. speed input: 9693.61 toks/s, output: 17.86 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1870.79it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.47it/s, est. speed input: 9693.10 toks/s, output: 14.95 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1933.75it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.45it/s, est. speed input: 9760.22 toks/s, output: 14.58 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1754.94it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.06it/s, est. speed input: 9700.74 toks/s, output: 10.75 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1835.58it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.66it/s, est. speed input: 9716.35 toks/s, output: 16.85 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1537.50it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.23it/s, est. speed input: 9650.35 toks/s, output: 12.51 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1675.04it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.77it/s, est. speed input: 9631.59 toks/s, output: 18.08 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1735.33it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.51it/s, est. speed input: 10020.14 toks/s, output: 15.32 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1736.77it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.31it/s, est. speed input: 9607.00 toks/s, output: 13.33 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1765.28it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.66it/s, est. speed input: 9606.18 toks/s, output: 16.92 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1839.61it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.48it/s, est. speed input: 10011.58 toks/s, output: 15.08 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1678.39it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.19it/s, est. speed input: 9923.39 toks/s, output: 12.06 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1771.99it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.27it/s, est. speed input: 9975.03 toks/s, output: 12.89 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1745.44it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.67it/s, est. speed input: 10072.52 toks/s, output: 16.99 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1724.63it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.62it/s, est. speed input: 9590.48 toks/s, output: 16.46 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1881.70it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.65it/s, est. speed input: 9719.73 toks/s, output: 16.77 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1652.60it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.11it/s, est. speed input: 9879.95 toks/s, output: 11.16 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1998.24it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.72it/s, est. speed input: 9931.53 toks/s, output: 17.47 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1608.86it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.29it/s, est. speed input: 9858.66 toks/s, output: 13.06 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1686.49it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.53it/s, est. speed input: 9935.30 toks/s, output: 15.60 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1794.74it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.43it/s, est. speed input: 9731.38 toks/s, output: 14.45 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1828.38it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.22it/s, est. speed input: 9777.43 toks/s, output: 12.31 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1651.30it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.55it/s, est. speed input: 9935.65 toks/s, output: 15.70 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1756.41it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.32it/s, est. speed input: 9789.61 toks/s, output: 13.32 toks/s]\n",
      "25it [00:18,  1.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task v|4|5|Y|20|sys_prompt10|5|0.01|ver6 실행 중: 3\n",
      "./result/sc_v_result_4_5_Y_20_sys_prompt10_5_0.01_ver6_3.csv\n",
      "/usr/share/d_ollama/data/q_output_code_y_74.csv\n",
      "VLLM\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2518180)\u001b[0;0m WARNING 11-26 14:19:25 [multiproc_worker_utils.py:273] Reducing Torch parallelism from 32 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "[W1126 14:19:29.318503081 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())\n",
      "[W1126 14:19:29.529522426 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())\n",
      "[W1126 14:19:29.623616495 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())\n",
      "[W1126 14:19:29.623646091 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3[Gloo] Rank \n",
      "[Gloo] Rank 02 is connected to  is connected to 33 peer ranks.  peer ranks. Expected number of connected peer ranks is : Expected number of connected peer ranks is : 33\n",
      "\n",
      "[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2518180)\u001b[0;0m \u001b[1;36m(EngineCore_DP0 pid=2518180)\u001b[0;0m \u001b[1;36m(EngineCore_DP0 pid=2518180)\u001b[0;0m \u001b[1;36m(EngineCore_DP0 pid=2518180)\u001b[0;0m WARNING 11-26 14:19:30 [custom_all_reduce.py:144] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 11-26 14:19:30 [custom_all_reduce.py:144] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 11-26 14:19:30 [custom_all_reduce.py:144] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 11-26 14:19:30 [custom_all_reduce.py:144] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2518180)\u001b[0;0m WARNING 11-26 14:19:30 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2518180)\u001b[0;0m \u001b[1;36m(EngineCore_DP0 pid=2518180)\u001b[0;0m WARNING 11-26 14:19:30 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "WARNING 11-26 14:19:30 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2518180)\u001b[0;0m WARNING 11-26 14:19:30 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s];0m \n",
      "Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.97it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  3.08it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  2.82it/s]\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2518180)\u001b[0;0m \u001b[1;36m(Worker_TP0 pid=2518190)\u001b[0;0m \n",
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 67/67 [00:05<00:00, 13.13it/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1376.54it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.82it/s, est. speed input: 8869.23 toks/s, output: 18.59 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1656.52it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.23it/s, est. speed input: 9791.51 toks/s, output: 12.50 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1541.46it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.11s/it, est. speed input: 9673.13 toks/s, output: 9.06 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1766.77it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.32it/s, est. speed input: 9563.29 toks/s, output: 13.36 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1778.75it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.89it/s, est. speed input: 10032.07 toks/s, output: 19.26 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1648.06it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.29it/s, est. speed input: 9766.89 toks/s, output: 13.11 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1733.90it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.18it/s, est. speed input: 9907.66 toks/s, output: 11.90 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1717.57it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.68it/s, est. speed input: 10070.08 toks/s, output: 17.03 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1585.75it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.09it/s, est. speed input: 10139.62 toks/s, output: 10.98 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1723.92it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.26it/s, est. speed input: 9725.60 toks/s, output: 12.72 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1794.74it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.61it/s, est. speed input: 9870.10 toks/s, output: 16.33 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1588.15it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.08it/s, est. speed input: 10141.26 toks/s, output: 10.94 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1628.22it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.02s/it, est. speed input: 9899.59 toks/s, output: 9.87 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1637.12it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.36it/s, est. speed input: 9739.47 toks/s, output: 13.81 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1458.89it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.62it/s, est. speed input: 9437.27 toks/s, output: 16.49 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1644.18it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.29it/s, est. speed input: 9740.61 toks/s, output: 13.11 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1501.18it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.09s/it, est. speed input: 9751.23 toks/s, output: 9.25 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1798.59it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.61it/s, est. speed input: 10063.83 toks/s, output: 16.35 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1595.40it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.65it/s, est. speed input: 9499.54 toks/s, output: 16.77 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1632.02it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.27it/s, est. speed input: 9949.21 toks/s, output: 12.82 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1527.42it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.20s/it, est. speed input: 9765.59 toks/s, output: 8.43 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1723.92it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.16it/s, est. speed input: 9699.55 toks/s, output: 11.70 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1798.59it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.83it/s, est. speed input: 9837.35 toks/s, output: 18.60 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1853.43it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.71it/s, est. speed input: 10174.55 toks/s, output: 17.36 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1665.07it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.48it/s, est. speed input: 9768.95 toks/s, output: 15.00 toks/s]\n",
      "25it [00:20,  1.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task v|4|5|Y|20|sys_prompt10|5|0.01|ver6 실행 중: 4\n",
      "./result/sc_v_result_4_5_Y_20_sys_prompt10_5_0.01_ver6_4.csv\n",
      "/usr/share/d_ollama/data/q_output_code_y_74.csv\n",
      "VLLM\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2522922)\u001b[0;0m WARNING 11-26 14:20:12 [multiproc_worker_utils.py:273] Reducing Torch parallelism from 32 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "[W1126 14:20:15.967979657 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())\n",
      "[W1126 14:20:15.967979677 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())\n",
      "[W1126 14:20:16.581234542 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())\n",
      "[W1126 14:20:16.586874067 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank 0[Gloo] Rank  is connected to 3 peer ranks. 1Expected number of connected peer ranks is :  is connected to 33 peer ranks. Expected number of connected peer ranks is : \n",
      "3\n",
      "[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2522922)\u001b[0;0m \u001b[1;36m(EngineCore_DP0 pid=2522922)\u001b[0;0m \u001b[1;36m(EngineCore_DP0 pid=2522922)\u001b[0;0m \u001b[1;36m(EngineCore_DP0 pid=2522922)\u001b[0;0m WARNING 11-26 14:20:16 [custom_all_reduce.py:144] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 11-26 14:20:16 [custom_all_reduce.py:144] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 11-26 14:20:16 [custom_all_reduce.py:144] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 11-26 14:20:16 [custom_all_reduce.py:144] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2522922)\u001b[0;0m \u001b[1;36m(EngineCore_DP0 pid=2522922)\u001b[0;0m \u001b[1;36m(EngineCore_DP0 pid=2522922)\u001b[0;0m \u001b[1;36m(EngineCore_DP0 pid=2522922)\u001b[0;0m WARNING 11-26 14:20:17 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "WARNING 11-26 14:20:17 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "WARNING 11-26 14:20:17 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "WARNING 11-26 14:20:17 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s];0m \n",
      "Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  2.08it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  3.19it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  2.92it/s]\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2522922)\u001b[0;0m \u001b[1;36m(Worker_TP0 pid=2522932)\u001b[0;0m \n",
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 67/67 [00:05<00:00, 12.71it/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 584.98it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.12it/s, est. speed input: 9259.74 toks/s, output: 11.28 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 808.00it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.23it/s, est. speed input: 9874.90 toks/s, output: 12.42 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1652.60it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.41it/s, est. speed input: 9793.97 toks/s, output: 14.32 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1435.91it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.06s/it, est. speed input: 9968.39 toks/s, output: 9.54 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1892.74it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.53it/s, est. speed input: 9500.49 toks/s, output: 15.48 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1684.46it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.09it/s, est. speed input: 9841.55 toks/s, output: 11.02 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1853.43it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.64it/s, est. speed input: 9722.52 toks/s, output: 16.61 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1640.96it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.32it/s, est. speed input: 9524.01 toks/s, output: 13.33 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1589.96it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.85it/s, est. speed input: 9641.47 toks/s, output: 18.79 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1856.71it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.72it/s, est. speed input: 9796.16 toks/s, output: 17.47 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1669.71it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.28it/s, est. speed input: 9739.15 toks/s, output: 12.93 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1630.76it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.84it/s, est. speed input: 9727.21 toks/s, output: 18.78 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1723.92it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.35it/s, est. speed input: 9619.91 toks/s, output: 13.63 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 666.29it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.25s/it, est. speed input: 9677.46 toks/s, output: 8.05 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1750.54it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.25it/s, est. speed input: 9987.53 toks/s, output: 12.66 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1564.46it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.46it/s, est. speed input: 10028.16 toks/s, output: 14.84 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1698.79it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.56it/s, est. speed input: 9562.27 toks/s, output: 15.85 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1693.98it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.19it/s, est. speed input: 9780.47 toks/s, output: 12.01 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1577.40it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.16it/s, est. speed input: 10167.40 toks/s, output: 11.68 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1781.02it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.34it/s, est. speed input: 10139.97 toks/s, output: 13.56 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1671.04it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.39it/s, est. speed input: 9993.69 toks/s, output: 14.04 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1742.54it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.41it/s, est. speed input: 9844.86 toks/s, output: 14.25 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1749.08it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.60it/s, est. speed input: 9661.00 toks/s, output: 16.26 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1860.83it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.53it/s, est. speed input: 10153.21 toks/s, output: 15.53 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1573.85it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.07it/s, est. speed input: 10259.65 toks/s, output: 10.82 toks/s]\n",
      "25it [00:20,  1.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task v|4|5|Y|20|sys_prompt10|5|0.01|ver6 실행 중: 5\n",
      "./result/sc_v_result_4_5_Y_20_sys_prompt10_5_0.01_ver6_5.csv\n",
      "/usr/share/d_ollama/data/q_output_code_y_74.csv\n",
      "VLLM\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2527982)\u001b[0;0m WARNING 11-26 14:21:00 [multiproc_worker_utils.py:273] Reducing Torch parallelism from 32 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "[W1126 14:21:03.268604635 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())\n",
      "[W1126 14:21:03.282168145 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())\n",
      "[W1126 14:21:03.312199645 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())\n",
      "[W1126 14:21:03.313208198 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank [Gloo] Rank 23 is connected to  is connected to 3 peer ranks. 3Expected number of connected peer ranks is : 3 peer ranks. \n",
      "Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank [Gloo] Rank 10 is connected to  is connected to 33 peer ranks. Expected number of connected peer ranks is :  peer ranks. 3Expected number of connected peer ranks is : \n",
      "3\n",
      "[Gloo] Rank 2 is connected to [Gloo] Rank 3 peer ranks. Expected number of connected peer ranks is : 33 is connected to 3\n",
      " peer ranks. Expected number of connected peer ranks is : 3\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2527982)\u001b[0;0m \u001b[1;36m(EngineCore_DP0 pid=2527982)\u001b[0;0m \u001b[1;36m(EngineCore_DP0 pid=2527982)\u001b[0;0m \u001b[1;36m(EngineCore_DP0 pid=2527982)\u001b[0;0m WARNING 11-26 14:21:03 [custom_all_reduce.py:144] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 11-26 14:21:03 [custom_all_reduce.py:144] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 11-26 14:21:03 [custom_all_reduce.py:144] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 11-26 14:21:03 [custom_all_reduce.py:144] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2527982)\u001b[0;0m \u001b[1;36m(EngineCore_DP0 pid=2527982)\u001b[0;0m \u001b[1;36m(EngineCore_DP0 pid=2527982)\u001b[0;0m \u001b[1;36m(EngineCore_DP0 pid=2527982)\u001b[0;0m WARNING 11-26 14:21:03 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "WARNING 11-26 14:21:03 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "WARNING 11-26 14:21:03 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "WARNING 11-26 14:21:03 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3[Gloo] Rank \n",
      "3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s];0m \n",
      "Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  2.02it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  3.20it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  2.93it/s]\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2527982)\u001b[0;0m \u001b[1;36m(Worker_TP0 pid=2527992)\u001b[0;0m \n",
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 67/67 [00:05<00:00, 12.90it/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 994.85it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.40it/s, est. speed input: 8832.39 toks/s, output: 14.16 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1348.22it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.58it/s, est. speed input: 9699.68 toks/s, output: 16.02 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1976.58it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.85it/s, est. speed input: 9723.61 toks/s, output: 18.82 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1955.39it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.88it/s, est. speed input: 9652.05 toks/s, output: 19.14 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1792.44it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.60it/s, est. speed input: 9782.06 toks/s, output: 16.22 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1855.89it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.51it/s, est. speed input: 9742.60 toks/s, output: 15.34 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1708.47it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.30it/s, est. speed input: 9870.33 toks/s, output: 13.20 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1807.89it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.38it/s, est. speed input: 9970.74 toks/s, output: 13.97 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1602.71it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.35it/s, est. speed input: 10040.83 toks/s, output: 13.66 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1467.57it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.35it/s, est. speed input: 9712.25 toks/s, output: 13.61 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1772.74it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.90it/s, est. speed input: 9565.70 toks/s, output: 19.36 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1598.44it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  2.18it/s, est. speed input: 9946.32 toks/s, output: 22.29 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1419.39it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.22it/s, est. speed input: 10214.49 toks/s, output: 12.31 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 901.81it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.26it/s, est. speed input: 10437.31 toks/s, output: 12.74 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1754.20it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.54it/s, est. speed input: 9899.26 toks/s, output: 15.61 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1664.41it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.09it/s, est. speed input: 9811.43 toks/s, output: 11.02 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1422.76it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.20it/s, est. speed input: 10215.36 toks/s, output: 12.14 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1787.85it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.52it/s, est. speed input: 9838.74 toks/s, output: 15.38 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1916.96it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.55it/s, est. speed input: 9939.32 toks/s, output: 15.76 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1594.79it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.30it/s, est. speed input: 10077.70 toks/s, output: 13.16 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1636.48it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.51it/s, est. speed input: 9884.93 toks/s, output: 15.27 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1757.88it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.22it/s, est. speed input: 9931.90 toks/s, output: 12.34 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1893.59it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.50it/s, est. speed input: 9805.92 toks/s, output: 15.17 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1688.53it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.14it/s, est. speed input: 10259.05 toks/s, output: 11.55 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1338.75it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.57it/s, est. speed input: 9664.08 toks/s, output: 15.92 toks/s]\n",
      "25it [00:18,  1.36it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task v|4|5|Y|20|sys_prompt10|5|0.01|ver6 실행 중: 6\n",
      "./result/sc_v_result_4_5_Y_20_sys_prompt10_5_0.01_ver6_6.csv\n",
      "/usr/share/d_ollama/data/q_output_code_y_74.csv\n",
      "VLLM\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2533213)\u001b[0;0m WARNING 11-26 14:21:44 [multiproc_worker_utils.py:273] Reducing Torch parallelism from 32 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "[W1126 14:21:47.235058778 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())\n",
      "[W1126 14:21:47.236900127 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())\n",
      "[W1126 14:21:47.613388221 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())\n",
      "[W1126 14:21:47.618062257 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank [Gloo] Rank 2 is connected to 33 peer ranks.  is connected to Expected number of connected peer ranks is : 33 peer ranks. \n",
      "Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank [Gloo] Rank 23 is connected to  is connected to 3 peer ranks. 3Expected number of connected peer ranks is :  peer ranks. 3Expected number of connected peer ranks is : 3\n",
      "\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2533213)\u001b[0;0m \u001b[1;36m(EngineCore_DP0 pid=2533213)\u001b[0;0m \u001b[1;36m(EngineCore_DP0 pid=2533213)\u001b[0;0m \u001b[1;36m(EngineCore_DP0 pid=2533213)\u001b[0;0m WARNING 11-26 14:21:47 [custom_all_reduce.py:144] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 11-26 14:21:47 [custom_all_reduce.py:144] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 11-26 14:21:47 [custom_all_reduce.py:144] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 11-26 14:21:47 [custom_all_reduce.py:144] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2533213)\u001b[0;0m WARNING 11-26 14:21:48 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2533213)\u001b[0;0m \u001b[1;36m(EngineCore_DP0 pid=2533213)\u001b[0;0m \u001b[1;36m(EngineCore_DP0 pid=2533213)\u001b[0;0m WARNING 11-26 14:21:48 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "WARNING 11-26 14:21:48 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "WARNING 11-26 14:21:48 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s];0m \n",
      "Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  2.05it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  3.14it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  2.89it/s]\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2533213)\u001b[0;0m \u001b[1;36m(Worker_TP0 pid=2533241)\u001b[0;0m \n",
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 67/67 [00:05<00:00, 12.69it/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1352.56it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.27it/s, est. speed input: 9106.25 toks/s, output: 12.81 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1891.88it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.46it/s, est. speed input: 9842.28 toks/s, output: 14.82 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 838.69it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.48it/s, est. speed input: 9668.41 toks/s, output: 14.98 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1635.21it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.72it/s, est. speed input: 9378.04 toks/s, output: 17.44 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1928.42it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.55it/s, est. speed input: 9778.46 toks/s, output: 15.70 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1715.46it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.18it/s, est. speed input: 9944.69 toks/s, output: 11.93 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1809.45it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.31it/s, est. speed input: 9960.97 toks/s, output: 13.29 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1796.28it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.74it/s, est. speed input: 9656.47 toks/s, output: 17.66 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1782.53it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.16it/s, est. speed input: 9749.83 toks/s, output: 11.72 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1686.49it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.17it/s, est. speed input: 10199.98 toks/s, output: 11.86 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1824.40it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.53it/s, est. speed input: 9765.27 toks/s, output: 15.52 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1625.70it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.57it/s, est. speed input: 9596.32 toks/s, output: 15.96 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1687.85it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.52it/s, est. speed input: 9734.57 toks/s, output: 15.40 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1769.75it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.32it/s, est. speed input: 9870.88 toks/s, output: 13.41 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1831.57it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  2.04it/s, est. speed input: 9572.38 toks/s, output: 20.77 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1774.99it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.39it/s, est. speed input: 10360.73 toks/s, output: 14.04 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1879.17it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.43it/s, est. speed input: 10195.25 toks/s, output: 14.46 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1851.79it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.34it/s, est. speed input: 9996.12 toks/s, output: 13.56 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1783.29it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.29it/s, est. speed input: 9823.77 toks/s, output: 13.02 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1397.64it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.92it/s, est. speed input: 10033.79 toks/s, output: 19.55 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1287.78it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.55it/s, est. speed input: 10206.89 toks/s, output: 15.68 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1200.09it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.22it/s, est. speed input: 9738.21 toks/s, output: 12.33 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1120.87it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.02s/it, est. speed input: 9843.92 toks/s, output: 9.93 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1236.53it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.30it/s, est. speed input: 9779.70 toks/s, output: 13.14 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1351.69it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.85it/s, est. speed input: 10592.04 toks/s, output: 18.78 toks/s]\n",
      "25it [00:18,  1.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task v|4|5|Y|20|sys_prompt10|5|0.01|ver6 실행 중: 7\n",
      "./result/sc_v_result_4_5_Y_20_sys_prompt10_5_0.01_ver6_7.csv\n",
      "/usr/share/d_ollama/data/q_output_code_y_74.csv\n",
      "VLLM\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2538530)\u001b[0;0m WARNING 11-26 14:22:29 [multiproc_worker_utils.py:273] Reducing Torch parallelism from 32 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "[W1126 14:22:32.110627828 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())\n",
      "[W1126 14:22:32.110627870 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())\n",
      "[W1126 14:22:32.117634802 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())\n",
      "[W1126 14:22:32.120871839 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2538530)\u001b[0;0m \u001b[1;36m(EngineCore_DP0 pid=2538530)\u001b[0;0m \u001b[1;36m(EngineCore_DP0 pid=2538530)\u001b[0;0m \u001b[1;36m(EngineCore_DP0 pid=2538530)\u001b[0;0m WARNING 11-26 14:22:32 [custom_all_reduce.py:144] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 11-26 14:22:32 [custom_all_reduce.py:144] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 11-26 14:22:32 [custom_all_reduce.py:144] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 11-26 14:22:32 [custom_all_reduce.py:144] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2538530)\u001b[0;0m WARNING 11-26 14:22:32 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2538530)\u001b[0;0m \u001b[1;36m(EngineCore_DP0 pid=2538530)\u001b[0;0m \u001b[1;36m(EngineCore_DP0 pid=2538530)\u001b[0;0m WARNING 11-26 14:22:32 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "WARNING 11-26 14:22:32 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "WARNING 11-26 14:22:32 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3[Gloo] Rank \n",
      "1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s];0m \n",
      "Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  2.00it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  3.07it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  2.83it/s]\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2538530)\u001b[0;0m \u001b[1;36m(Worker_TP0 pid=2538540)\u001b[0;0m \n",
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 67/67 [00:05<00:00, 12.85it/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1333.64it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.23it/s, est. speed input: 9106.56 toks/s, output: 12.43 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1181.16it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.15it/s, est. speed input: 9891.93 toks/s, output: 11.54 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1190.89it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.14it/s, est. speed input: 9897.78 toks/s, output: 11.56 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1800.90it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.74it/s, est. speed input: 9896.24 toks/s, output: 17.57 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1353.00it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.69it/s, est. speed input: 9899.21 toks/s, output: 17.06 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1148.50it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.03it/s, est. speed input: 9644.68 toks/s, output: 10.34 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1305.42it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.52it/s, est. speed input: 10021.79 toks/s, output: 15.45 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 908.45it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.75it/s, est. speed input: 9819.07 toks/s, output: 17.73 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1577.40it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.39it/s, est. speed input: 9770.59 toks/s, output: 14.10 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1697.41it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.31it/s, est. speed input: 10107.28 toks/s, output: 13.30 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 748.58it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.01s/it, est. speed input: 9852.02 toks/s, output: 10.03 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1577.40it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.13it/s, est. speed input: 10495.45 toks/s, output: 11.41 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1562.12it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.81it/s, est. speed input: 10338.03 toks/s, output: 18.44 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1590.56it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.06it/s, est. speed input: 9820.66 toks/s, output: 10.70 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1448.81it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.11s/it, est. speed input: 10000.42 toks/s, output: 9.06 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1691.93it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.23it/s, est. speed input: 9784.70 toks/s, output: 12.46 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1854.25it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.47it/s, est. speed input: 9765.45 toks/s, output: 14.94 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1651.95it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.25it/s, est. speed input: 9857.47 toks/s, output: 12.69 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1718.98it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.22it/s, est. speed input: 9608.79 toks/s, output: 12.30 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 714.17it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.11s/it, est. speed input: 9831.21 toks/s, output: 9.11 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1609.48it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.45it/s, est. speed input: 9631.61 toks/s, output: 14.75 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1611.95it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.51it/s, est. speed input: 9729.54 toks/s, output: 15.31 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1547.14it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.01s/it, est. speed input: 9867.28 toks/s, output: 10.04 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1700.16it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.22it/s, est. speed input: 9875.88 toks/s, output: 12.32 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1728.90it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.43it/s, est. speed input: 10227.22 toks/s, output: 14.53 toks/s]\n",
      "25it [00:21,  1.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task v|4|5|Y|20|sys_prompt10|5|0.01|ver6 실행 중: 8\n",
      "./result/sc_v_result_4_5_Y_20_sys_prompt10_5_0.01_ver6_8.csv\n",
      "/usr/share/d_ollama/data/q_output_code_y_74.csv\n",
      "VLLM\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2543970)\u001b[0;0m WARNING 11-26 14:23:15 [multiproc_worker_utils.py:273] Reducing Torch parallelism from 32 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "[W1126 14:23:18.943817248 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())\n",
      "[W1126 14:23:18.944187603 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())\n",
      "[W1126 14:23:19.524213635 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())\n",
      "[W1126 14:23:19.533141413 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3[Gloo] Rank \n",
      "2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2543970)\u001b[0;0m \u001b[1;36m(EngineCore_DP0 pid=2543970)\u001b[0;0m \u001b[1;36m(EngineCore_DP0 pid=2543970)\u001b[0;0m WARNING 11-26 14:23:19 [custom_all_reduce.py:144] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2543970)\u001b[0;0m WARNING 11-26 14:23:19 [custom_all_reduce.py:144] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 11-26 14:23:19 [custom_all_reduce.py:144] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 11-26 14:23:19 [custom_all_reduce.py:144] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2543970)\u001b[0;0m \u001b[1;36m(EngineCore_DP0 pid=2543970)\u001b[0;0m WARNING 11-26 14:23:20 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "WARNING 11-26 14:23:20 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2543970)\u001b[0;0m \u001b[1;36m(EngineCore_DP0 pid=2543970)\u001b[0;0m WARNING 11-26 14:23:20 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "WARNING 11-26 14:23:20 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s];0m \n",
      "Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  2.07it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  3.23it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  2.97it/s]\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2543970)\u001b[0;0m \u001b[1;36m(Worker_TP0 pid=2543997)\u001b[0;0m \n",
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 67/67 [00:05<00:00, 12.87it/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 960.67it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.40it/s, est. speed input: 9112.06 toks/s, output: 14.17 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1534.69it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.08s/it, est. speed input: 9683.04 toks/s, output: 9.29 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1267.16it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.32it/s, est. speed input: 9605.06 toks/s, output: 13.29 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1292.15it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.39it/s, est. speed input: 9617.47 toks/s, output: 14.09 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1653.91it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.47it/s, est. speed input: 9560.28 toks/s, output: 14.91 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1565.62it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.52it/s, est. speed input: 9421.48 toks/s, output: 15.46 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 911.61it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.66it/s, est. speed input: 9693.42 toks/s, output: 16.89 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1795.51it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.82it/s, est. speed input: 9386.47 toks/s, output: 18.49 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 950.87it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.89it/s, est. speed input: 10483.53 toks/s, output: 19.19 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 802.58it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.16it/s, est. speed input: 9913.75 toks/s, output: 11.76 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1781.78it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.32it/s, est. speed input: 9703.84 toks/s, output: 13.41 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1895.30it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.96it/s, est. speed input: 9556.88 toks/s, output: 19.92 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1829.98it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.32it/s, est. speed input: 9672.53 toks/s, output: 13.36 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1611.95it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.14s/it, est. speed input: 9667.98 toks/s, output: 8.82 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1778.00it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.97it/s, est. speed input: 9779.53 toks/s, output: 20.08 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1675.04it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.84it/s, est. speed input: 9796.89 toks/s, output: 18.76 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1792.44it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.72it/s, est. speed input: 9696.20 toks/s, output: 17.51 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1826.79it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.68it/s, est. speed input: 10111.09 toks/s, output: 17.08 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1562.12it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.45it/s, est. speed input: 10076.99 toks/s, output: 14.68 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1745.44it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  2.10it/s, est. speed input: 10201.42 toks/s, output: 21.45 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1908.24it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.69it/s, est. speed input: 10340.41 toks/s, output: 17.16 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1680.41it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.20it/s, est. speed input: 10094.70 toks/s, output: 12.12 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1999.19it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.64it/s, est. speed input: 9601.74 toks/s, output: 16.63 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1880.85it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.72it/s, est. speed input: 10387.80 toks/s, output: 17.48 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1790.14it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.54it/s, est. speed input: 10054.40 toks/s, output: 15.67 toks/s]\n",
      "25it [00:17,  1.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task v|4|5|Y|20|sys_prompt10|5|0.01|ver6 실행 중: 9\n",
      "./result/sc_v_result_4_5_Y_20_sys_prompt10_5_0.01_ver6_9.csv\n",
      "/usr/share/d_ollama/data/q_output_code_y_74.csv\n",
      "VLLM\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2553160)\u001b[0;0m WARNING 11-26 14:24:00 [multiproc_worker_utils.py:273] Reducing Torch parallelism from 32 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "[W1126 14:24:03.354747307 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())\n",
      "[W1126 14:24:03.361008114 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())\n",
      "[W1126 14:24:03.815005157 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())\n",
      "[W1126 14:24:03.820288259 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2553160)\u001b[0;0m \u001b[1;36m(EngineCore_DP0 pid=2553160)\u001b[0;0m \u001b[1;36m(EngineCore_DP0 pid=2553160)\u001b[0;0m \u001b[1;36m(EngineCore_DP0 pid=2553160)\u001b[0;0m WARNING 11-26 14:24:04 [custom_all_reduce.py:144] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 11-26 14:24:04 [custom_all_reduce.py:144] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 11-26 14:24:04 [custom_all_reduce.py:144] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 11-26 14:24:04 [custom_all_reduce.py:144] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2553160)\u001b[0;0m \u001b[1;36m(EngineCore_DP0 pid=2553160)\u001b[0;0m \u001b[1;36m(EngineCore_DP0 pid=2553160)\u001b[0;0m \u001b[1;36m(EngineCore_DP0 pid=2553160)\u001b[0;0m WARNING 11-26 14:24:04 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "WARNING 11-26 14:24:04 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "WARNING 11-26 14:24:04 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "WARNING 11-26 14:24:04 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s];0m \n",
      "Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  2.03it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  3.16it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  2.90it/s]\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2553160)\u001b[0;0m \u001b[1;36m(Worker_TP0 pid=2553410)\u001b[0;0m \n",
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 67/67 [00:05<00:00, 12.80it/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1291.75it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.28it/s, est. speed input: 9101.86 toks/s, output: 12.92 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1416.52it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.05s/it, est. speed input: 9758.00 toks/s, output: 9.63 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 911.21it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.73it/s, est. speed input: 9708.77 toks/s, output: 17.52 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1683.78it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 9565.80 toks/s, output: 10.25 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1818.87it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.25it/s, est. speed input: 9679.08 toks/s, output: 12.70 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1427.12it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.00s/it, est. speed input: 9800.77 toks/s, output: 10.08 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1657.17it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.25it/s, est. speed input: 9975.39 toks/s, output: 12.63 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 753.02it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.10it/s, est. speed input: 9803.43 toks/s, output: 11.13 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 841.55it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.30it/s, est. speed input: 9717.18 toks/s, output: 13.13 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1761.57it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.53it/s, est. speed input: 9507.83 toks/s, output: 15.57 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 899.29it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.62it/s, est. speed input: 9876.43 toks/s, output: 16.47 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1653.91it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.03it/s, est. speed input: 10017.00 toks/s, output: 10.41 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1706.39it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.55it/s, est. speed input: 10110.48 toks/s, output: 15.74 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1680.41it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.06it/s, est. speed input: 9846.06 toks/s, output: 10.68 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1620.05it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.13it/s, est. speed input: 10052.78 toks/s, output: 11.39 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1788.62it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.24it/s, est. speed input: 9694.22 toks/s, output: 12.57 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1560.96it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.31it/s, est. speed input: 9963.67 toks/s, output: 13.25 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1544.86it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.40it/s, est. speed input: 9686.02 toks/s, output: 14.22 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1865.79it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.65it/s, est. speed input: 9948.38 toks/s, output: 16.69 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1868.29it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.56it/s, est. speed input: 9995.74 toks/s, output: 15.79 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 809.24it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.21it/s, est. speed input: 10091.18 toks/s, output: 12.27 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1582.76it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.19s/it, est. speed input: 9593.31 toks/s, output: 8.47 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1665.07it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.57it/s, est. speed input: 9761.23 toks/s, output: 15.88 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1670.37it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.58it/s, est. speed input: 9685.30 toks/s, output: 16.02 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1701.54it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.27it/s, est. speed input: 10092.87 toks/s, output: 12.85 toks/s]\n",
      "25it [00:21,  1.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task v|4|5|Y|20|sys_prompt10|5|0.01|ver6 실행 중: 10\n",
      "./result/sc_v_result_4_5_Y_20_sys_prompt10_5_0.01_ver6_10.csv\n",
      "/usr/share/d_ollama/data/q_output_code_y_74.csv\n",
      "VLLM\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2558336)\u001b[0;0m WARNING 11-26 14:24:47 [multiproc_worker_utils.py:273] Reducing Torch parallelism from 32 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "[W1126 14:24:50.685754418 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())\n",
      "[W1126 14:24:50.690143758 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())\n",
      "[W1126 14:24:51.405954019 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())\n",
      "[W1126 14:24:51.413432601 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3[Gloo] Rank \n",
      "1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2558336)\u001b[0;0m \u001b[1;36m(EngineCore_DP0 pid=2558336)\u001b[0;0m WARNING 11-26 14:24:51 [custom_all_reduce.py:144] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2558336)\u001b[0;0m WARNING 11-26 14:24:51 [custom_all_reduce.py:144] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 11-26 14:24:51 [custom_all_reduce.py:144] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2558336)\u001b[0;0m WARNING 11-26 14:24:51 [custom_all_reduce.py:144] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2558336)\u001b[0;0m WARNING 11-26 14:24:51 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2558336)\u001b[0;0m \u001b[1;36m(EngineCore_DP0 pid=2558336)\u001b[0;0m \u001b[1;36m(EngineCore_DP0 pid=2558336)\u001b[0;0m WARNING 11-26 14:24:51 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "WARNING 11-26 14:24:51 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "WARNING 11-26 14:24:51 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : [Gloo] Rank 0\n",
      "0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s];0m \n",
      "Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  2.07it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  3.16it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  2.91it/s]\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2558336)\u001b[0;0m \u001b[1;36m(Worker_TP0 pid=2558346)\u001b[0;0m \n",
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 67/67 [00:05<00:00, 12.91it/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1356.94it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.14it/s, est. speed input: 9272.98 toks/s, output: 11.55 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1882.54it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.47it/s, est. speed input: 9747.40 toks/s, output: 14.85 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1825.99it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.10it/s, est. speed input: 9686.00 toks/s, output: 11.17 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 2032.12it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.48it/s, est. speed input: 9877.54 toks/s, output: 15.00 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1675.04it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.43it/s, est. speed input: 9754.00 toks/s, output: 14.48 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1951.75it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.33it/s, est. speed input: 9960.78 toks/s, output: 13.44 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1797.82it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.05it/s, est. speed input: 9726.15 toks/s, output: 10.59 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 2130.17it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.91it/s, est. speed input: 10352.54 toks/s, output: 19.37 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1872.46it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.74it/s, est. speed input: 9930.85 toks/s, output: 17.66 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1547.14it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.45it/s, est. speed input: 9849.89 toks/s, output: 14.73 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1822.03it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.12it/s, est. speed input: 9947.93 toks/s, output: 11.36 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1923.11it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.29it/s, est. speed input: 9871.08 toks/s, output: 13.02 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1711.26it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.36it/s, est. speed input: 10108.45 toks/s, output: 13.81 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1812.58it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.03it/s, est. speed input: 10027.22 toks/s, output: 10.45 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1656.52it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.64it/s, est. speed input: 9854.24 toks/s, output: 16.60 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1909.97it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.69it/s, est. speed input: 9836.60 toks/s, output: 17.07 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 876.00it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.20it/s, est. speed input: 10268.45 toks/s, output: 12.17 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1041.03it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  2.10it/s, est. speed input: 10742.53 toks/s, output: 21.44 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1644.18it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.28it/s, est. speed input: 10369.70 toks/s, output: 12.96 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 2020.38it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.43it/s, est. speed input: 9885.21 toks/s, output: 14.49 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1771.24it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.01it/s, est. speed input: 9693.31 toks/s, output: 10.20 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1779.51it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.37it/s, est. speed input: 9845.52 toks/s, output: 13.90 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1790.91it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.70it/s, est. speed input: 9748.88 toks/s, output: 17.23 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1807.11it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.60it/s, est. speed input: 10343.02 toks/s, output: 16.26 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1907.37it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.30it/s, est. speed input: 10039.51 toks/s, output: 13.20 toks/s]\n",
      "25it [00:19,  1.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task v|4|5|Y|20|sys_prompt10|5|0.01|ver6 실행 중: 11\n",
      "./result/sc_v_result_4_5_Y_20_sys_prompt10_5_0.01_ver6_11.csv\n",
      "/usr/share/d_ollama/data/q_output_code_y_74.csv\n",
      "VLLM\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2563569)\u001b[0;0m WARNING 11-26 14:25:33 [multiproc_worker_utils.py:273] Reducing Torch parallelism from 32 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "[W1126 14:25:36.702911194 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())\n",
      "[W1126 14:25:36.709012230 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())\n",
      "[W1126 14:25:36.717767922 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())\n",
      "[W1126 14:25:36.718899663 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank 3 is connected to 3 peer ranks. [Gloo] Rank Expected number of connected peer ranks is : 32\n",
      " is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2563569)\u001b[0;0m \u001b[1;36m(EngineCore_DP0 pid=2563569)\u001b[0;0m \u001b[1;36m(EngineCore_DP0 pid=2563569)\u001b[0;0m \u001b[1;36m(EngineCore_DP0 pid=2563569)\u001b[0;0m WARNING 11-26 14:25:37 [custom_all_reduce.py:144] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 11-26 14:25:37 [custom_all_reduce.py:144] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 11-26 14:25:37 [custom_all_reduce.py:144] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 11-26 14:25:37 [custom_all_reduce.py:144] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2563569)\u001b[0;0m \u001b[1;36m(EngineCore_DP0 pid=2563569)\u001b[0;0m \u001b[1;36m(EngineCore_DP0 pid=2563569)\u001b[0;0m \u001b[1;36m(EngineCore_DP0 pid=2563569)\u001b[0;0m WARNING 11-26 14:25:37 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "WARNING 11-26 14:25:37 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "WARNING 11-26 14:25:37 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "WARNING 11-26 14:25:37 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank 2 is connected to [Gloo] Rank 3 peer ranks. Expected number of connected peer ranks is : 03 is connected to 3\n",
      " peer ranks. Expected number of connected peer ranks is : 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s];0m \n",
      "Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.97it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  3.13it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  2.86it/s]\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2563569)\u001b[0;0m \u001b[1;36m(Worker_TP0 pid=2563579)\u001b[0;0m \n",
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 67/67 [00:05<00:00, 13.29it/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1471.17it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.43it/s, est. speed input: 8915.87 toks/s, output: 14.52 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1896.16it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.26it/s, est. speed input: 9910.07 toks/s, output: 12.79 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1985.94it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.62it/s, est. speed input: 9808.57 toks/s, output: 16.40 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1825.99it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.81it/s, est. speed input: 9918.33 toks/s, output: 18.37 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1937.32it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.49it/s, est. speed input: 10257.72 toks/s, output: 15.11 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 2089.84it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.67it/s, est. speed input: 9887.42 toks/s, output: 16.98 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1653.25it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.80it/s, est. speed input: 9958.82 toks/s, output: 18.25 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1814.15it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.63it/s, est. speed input: 9888.00 toks/s, output: 16.49 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1857.53it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.33it/s, est. speed input: 9687.43 toks/s, output: 13.47 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1764.54it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.50it/s, est. speed input: 10159.94 toks/s, output: 15.23 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 2033.11it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.67it/s, est. speed input: 9899.86 toks/s, output: 16.95 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1989.71it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.63it/s, est. speed input: 9916.06 toks/s, output: 16.52 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1840.41it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.31it/s, est. speed input: 9800.14 toks/s, output: 13.29 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1810.23it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.79it/s, est. speed input: 10033.58 toks/s, output: 18.19 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 833.20it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.33it/s, est. speed input: 10445.68 toks/s, output: 13.42 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1885.93it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.54it/s, est. speed input: 10161.27 toks/s, output: 15.61 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1779.51it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.45it/s, est. speed input: 9759.53 toks/s, output: 14.65 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1957.21it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.49it/s, est. speed input: 10242.28 toks/s, output: 15.13 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1963.63it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.35it/s, est. speed input: 10059.44 toks/s, output: 13.64 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1891.03it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.24it/s, est. speed input: 9686.05 toks/s, output: 12.58 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1743.99it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.22it/s, est. speed input: 10484.98 toks/s, output: 12.34 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1695.35it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.06it/s, est. speed input: 10016.78 toks/s, output: 10.75 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1656.52it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.57it/s, est. speed input: 10718.54 toks/s, output: 15.90 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1595.40it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.00s/it, est. speed input: 10001.58 toks/s, output: 10.06 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1584.55it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.07s/it, est. speed input: 9635.60 toks/s, output: 9.40 toks/s]\n",
      "25it [00:18,  1.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task v|4|5|Y|20|sys_prompt10|5|0.01|ver6 실행 중: 12\n",
      "./result/sc_v_result_4_5_Y_20_sys_prompt10_5_0.01_ver6_12.csv\n",
      "/usr/share/d_ollama/data/q_output_code_y_74.csv\n",
      "VLLM\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2568870)\u001b[0;0m WARNING 11-26 14:26:17 [multiproc_worker_utils.py:273] Reducing Torch parallelism from 32 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "[W1126 14:26:20.379408984 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())\n",
      "[W1126 14:26:20.380709782 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())\n",
      "[W1126 14:26:20.392889171 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())\n",
      "[W1126 14:26:20.402864411 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2568870)\u001b[0;0m \u001b[1;36m(EngineCore_DP0 pid=2568870)\u001b[0;0m \u001b[1;36m(EngineCore_DP0 pid=2568870)\u001b[0;0m \u001b[1;36m(EngineCore_DP0 pid=2568870)\u001b[0;0m WARNING 11-26 14:26:20 [custom_all_reduce.py:144] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 11-26 14:26:20 [custom_all_reduce.py:144] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 11-26 14:26:20 [custom_all_reduce.py:144] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 11-26 14:26:20 [custom_all_reduce.py:144] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2568870)\u001b[0;0m \u001b[1;36m(EngineCore_DP0 pid=2568870)\u001b[0;0m \u001b[1;36m(EngineCore_DP0 pid=2568870)\u001b[0;0m \u001b[1;36m(EngineCore_DP0 pid=2568870)\u001b[0;0m WARNING 11-26 14:26:20 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "WARNING 11-26 14:26:20 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "WARNING 11-26 14:26:20 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "WARNING 11-26 14:26:20 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3[Gloo] Rank \n",
      "2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s];0m \n",
      "Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  2.02it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  3.15it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  2.89it/s]\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2568870)\u001b[0;0m \u001b[1;36m(Worker_TP0 pid=2568880)\u001b[0;0m \n",
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 67/67 [00:05<00:00, 13.24it/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1296.54it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.16it/s, est. speed input: 9147.40 toks/s, output: 11.71 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1713.36it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.31it/s, est. speed input: 9788.51 toks/s, output: 13.31 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1717.57it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.90it/s, est. speed input: 9594.48 toks/s, output: 19.31 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1600.27it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.05s/it, est. speed input: 9814.12 toks/s, output: 9.63 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1697.41it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.23it/s, est. speed input: 9788.15 toks/s, output: 12.47 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1610.72it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.67it/s, est. speed input: 10531.75 toks/s, output: 16.90 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1538.07it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.47it/s, est. speed input: 9864.95 toks/s, output: 14.89 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1675.04it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.06it/s, est. speed input: 9754.62 toks/s, output: 10.75 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 919.40it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.53it/s, est. speed input: 9918.74 toks/s, output: 15.55 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1874.97it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.92it/s, est. speed input: 9698.82 toks/s, output: 19.50 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1648.70it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.08s/it, est. speed input: 9794.79 toks/s, output: 9.34 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1738.93it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.21it/s, est. speed input: 10042.28 toks/s, output: 12.25 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1681.08it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.67it/s, est. speed input: 10182.22 toks/s, output: 16.95 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1745.44it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.32it/s, est. speed input: 10055.04 toks/s, output: 13.41 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1594.79it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.78it/s, est. speed input: 9528.98 toks/s, output: 18.04 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1850.16it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.46it/s, est. speed input: 10162.02 toks/s, output: 14.77 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1963.63it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.45it/s, est. speed input: 10034.06 toks/s, output: 14.73 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 902.19it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  2.05it/s, est. speed input: 10143.93 toks/s, output: 20.93 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1603.33it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.05it/s, est. speed input: 9838.45 toks/s, output: 10.55 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1646.12it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.30it/s, est. speed input: 9976.78 toks/s, output: 13.12 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 940.22it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.84it/s, est. speed input: 9955.49 toks/s, output: 18.75 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1902.18it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.23it/s, est. speed input: 9820.74 toks/s, output: 12.43 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1728.90it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.22it/s, est. speed input: 10132.96 toks/s, output: 12.29 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 853.89it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.92it/s, est. speed input: 10796.96 toks/s, output: 19.76 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1421.80it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.29it/s, est. speed input: 10028.60 toks/s, output: 13.06 toks/s]\n",
      "25it [00:19,  1.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task v|4|5|Y|20|sys_prompt10|5|0.01|ver6 실행 중: 13\n",
      "./result/sc_v_result_4_5_Y_20_sys_prompt10_5_0.01_ver6_13.csv\n",
      "/usr/share/d_ollama/data/q_output_code_y_74.csv\n",
      "VLLM\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2574165)\u001b[0;0m WARNING 11-26 14:27:01 [multiproc_worker_utils.py:273] Reducing Torch parallelism from 32 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "[W1126 14:27:05.981740037 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())\n",
      "[W1126 14:27:05.277113734 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())\n",
      "[W1126 14:27:05.354139406 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())\n",
      "[W1126 14:27:05.362066323 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank [Gloo] Rank 21 is connected to  is connected to 33 peer ranks.  peer ranks. Expected number of connected peer ranks is : Expected number of connected peer ranks is : 33\n",
      "\n",
      "[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2574165)\u001b[0;0m \u001b[1;36m(EngineCore_DP0 pid=2574165)\u001b[0;0m \u001b[1;36m(EngineCore_DP0 pid=2574165)\u001b[0;0m \u001b[1;36m(EngineCore_DP0 pid=2574165)\u001b[0;0m WARNING 11-26 14:27:05 [custom_all_reduce.py:144] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 11-26 14:27:05 [custom_all_reduce.py:144] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 11-26 14:27:05 [custom_all_reduce.py:144] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 11-26 14:27:05 [custom_all_reduce.py:144] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2574165)\u001b[0;0m \u001b[1;36m(EngineCore_DP0 pid=2574165)\u001b[0;0m WARNING 11-26 14:27:05 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2574165)\u001b[0;0m WARNING 11-26 14:27:05 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2574165)\u001b[0;0m WARNING 11-26 14:27:05 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "WARNING 11-26 14:27:05 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : [Gloo] Rank 32\n",
      " is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s];0m \n",
      "Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  2.09it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  3.20it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  2.95it/s]\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2574165)\u001b[0;0m \u001b[1;36m(Worker_TP0 pid=2574175)\u001b[0;0m \n",
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 67/67 [00:04<00:00, 13.51it/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1203.88it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.09it/s, est. speed input: 9286.64 toks/s, output: 11.01 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 2090.88it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.68it/s, est. speed input: 9844.76 toks/s, output: 17.00 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1162.82it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.74it/s, est. speed input: 9958.35 toks/s, output: 17.62 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1877.49it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.36it/s, est. speed input: 10169.10 toks/s, output: 13.76 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1197.35it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.09it/s, est. speed input: 10145.54 toks/s, output: 10.96 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1348.65it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.54it/s, est. speed input: 10306.34 toks/s, output: 15.61 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1321.46it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.40it/s, est. speed input: 9647.30 toks/s, output: 14.12 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1302.58it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.33it/s, est. speed input: 9995.91 toks/s, output: 13.42 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1209.08it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.19it/s, est. speed input: 10093.54 toks/s, output: 11.95 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1233.98it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.13it/s, est. speed input: 9843.89 toks/s, output: 11.41 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1716.16it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.39it/s, est. speed input: 9825.10 toks/s, output: 14.01 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1463.47it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.81it/s, est. speed input: 10203.60 toks/s, output: 18.39 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1835.58it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.11it/s, est. speed input: 9698.91 toks/s, output: 11.17 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1312.36it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.46it/s, est. speed input: 9841.34 toks/s, output: 14.73 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1864.96it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.20it/s, est. speed input: 9856.46 toks/s, output: 12.11 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1445.31it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.08s/it, est. speed input: 9807.50 toks/s, output: 9.29 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1473.24it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.91it/s, est. speed input: 10204.72 toks/s, output: 19.40 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1826.79it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.21it/s, est. speed input: 9989.83 toks/s, output: 12.25 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1363.11it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.46it/s, est. speed input: 9853.94 toks/s, output: 14.72 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1631.39it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.34it/s, est. speed input: 10093.32 toks/s, output: 13.56 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1655.86it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.55it/s, est. speed input: 9827.77 toks/s, output: 15.75 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1932.86it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.77it/s, est. speed input: 10193.85 toks/s, output: 17.97 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 709.82it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.07s/it, est. speed input: 10086.33 toks/s, output: 9.43 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1651.95it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.62it/s, est. speed input: 9766.04 toks/s, output: 16.46 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1862.48it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.47it/s, est. speed input: 10309.87 toks/s, output: 14.77 toks/s]\n",
      "25it [00:19,  1.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task v|4|5|Y|20|sys_prompt10|5|0.01|ver6 실행 중: 14\n",
      "./result/sc_v_result_4_5_Y_20_sys_prompt10_5_0.01_ver6_14.csv\n",
      "/usr/share/d_ollama/data/q_output_code_y_74.csv\n",
      "VLLM\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2579457)\u001b[0;0m WARNING 11-26 14:27:47 [multiproc_worker_utils.py:273] Reducing Torch parallelism from 32 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "[W1126 14:27:50.491025589 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())\n",
      "[W1126 14:27:50.733902645 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())\n",
      "[W1126 14:27:50.824751118 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())\n",
      "[W1126 14:27:50.826199554 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : [Gloo] Rank 3\n",
      "3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2579457)\u001b[0;0m \u001b[1;36m(EngineCore_DP0 pid=2579457)\u001b[0;0m \u001b[1;36m(EngineCore_DP0 pid=2579457)\u001b[0;0m \u001b[1;36m(EngineCore_DP0 pid=2579457)\u001b[0;0m WARNING 11-26 14:27:51 [custom_all_reduce.py:144] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 11-26 14:27:51 [custom_all_reduce.py:144] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 11-26 14:27:51 [custom_all_reduce.py:144] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 11-26 14:27:51 [custom_all_reduce.py:144] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2579457)\u001b[0;0m WARNING 11-26 14:27:51 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2579457)\u001b[0;0m \u001b[1;36m(EngineCore_DP0 pid=2579457)\u001b[0;0m \u001b[1;36m(EngineCore_DP0 pid=2579457)\u001b[0;0m WARNING 11-26 14:27:51 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "WARNING 11-26 14:27:51 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "WARNING 11-26 14:27:51 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s];0m \n",
      "Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  2.01it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  3.15it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  2.89it/s]\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2579457)\u001b[0;0m \u001b[1;36m(Worker_TP0 pid=2579467)\u001b[0;0m \n",
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 67/67 [00:04<00:00, 13.53it/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1258.79it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.54it/s, est. speed input: 9070.58 toks/s, output: 15.60 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1728.90it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.50it/s, est. speed input: 9858.13 toks/s, output: 15.14 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1309.90it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.32it/s, est. speed input: 9686.44 toks/s, output: 13.39 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1813.36it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.47it/s, est. speed input: 9682.22 toks/s, output: 14.87 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1856.71it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.19it/s, est. speed input: 9752.46 toks/s, output: 12.04 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1694.67it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.90it/s, est. speed input: 9903.27 toks/s, output: 19.29 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1860.83it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.84it/s, est. speed input: 9764.90 toks/s, output: 18.73 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 697.19it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.26it/s, est. speed input: 10161.43 toks/s, output: 12.69 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 2113.00it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.86it/s, est. speed input: 9758.21 toks/s, output: 18.88 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 2029.17it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.50it/s, est. speed input: 9905.52 toks/s, output: 15.18 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1634.57it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.12it/s, est. speed input: 9748.44 toks/s, output: 11.32 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1551.72it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.05s/it, est. speed input: 10239.30 toks/s, output: 9.60 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1764.54it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.03it/s, est. speed input: 9830.48 toks/s, output: 10.35 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1778.75it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.18it/s, est. speed input: 10025.92 toks/s, output: 11.90 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1929.30it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.70it/s, est. speed input: 10270.07 toks/s, output: 17.23 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1188.52it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.13it/s, est. speed input: 9911.01 toks/s, output: 11.34 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 826.79it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.69it/s, est. speed input: 9965.16 toks/s, output: 17.15 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1778.00it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.51it/s, est. speed input: 9891.86 toks/s, output: 15.29 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1587.55it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.62it/s, est. speed input: 9911.38 toks/s, output: 16.45 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1580.37it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.49it/s, est. speed input: 9996.53 toks/s, output: 15.16 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1711.96it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.42it/s, est. speed input: 9762.69 toks/s, output: 14.37 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1663.75it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  2.05it/s, est. speed input: 10345.31 toks/s, output: 20.90 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1752.74it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.20it/s, est. speed input: 10172.15 toks/s, output: 12.16 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 2027.21it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.55it/s, est. speed input: 10183.30 toks/s, output: 15.75 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1685.81it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.05s/it, est. speed input: 9881.90 toks/s, output: 9.60 toks/s]\n",
      "25it [00:19,  1.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task v|4|5|Y|20|sys_prompt10|5|0.01|ver6 실행 중: 15\n",
      "./result/sc_v_result_4_5_Y_20_sys_prompt10_5_0.01_ver6_15.csv\n",
      "/usr/share/d_ollama/data/q_output_code_y_74.csv\n",
      "VLLM\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2585051)\u001b[0;0m WARNING 11-26 14:28:32 [multiproc_worker_utils.py:273] Reducing Torch parallelism from 32 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "[W1126 14:28:35.445627716 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())\n",
      "[W1126 14:28:35.705790291 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())\n",
      "[W1126 14:28:36.109506737 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())\n",
      "[W1126 14:28:36.116827648 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank 3 is connected to 3[Gloo] Rank  peer ranks. Expected number of connected peer ranks is : 23 is connected to 3\n",
      " peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank [Gloo] Rank 23 is connected to 3 is connected to  peer ranks. Expected number of connected peer ranks is : 33 peer ranks. \n",
      "Expected number of connected peer ranks is : 3\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2585051)\u001b[0;0m \u001b[1;36m(EngineCore_DP0 pid=2585051)\u001b[0;0m \u001b[1;36m(EngineCore_DP0 pid=2585051)\u001b[0;0m WARNING 11-26 14:28:36 [custom_all_reduce.py:144] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2585051)\u001b[0;0m WARNING 11-26 14:28:36 [custom_all_reduce.py:144] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 11-26 14:28:36 [custom_all_reduce.py:144] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 11-26 14:28:36 [custom_all_reduce.py:144] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2585051)\u001b[0;0m WARNING 11-26 14:28:36 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2585051)\u001b[0;0m WARNING 11-26 14:28:36 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2585051)\u001b[0;0m \u001b[1;36m(EngineCore_DP0 pid=2585051)\u001b[0;0m WARNING 11-26 14:28:36 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "WARNING 11-26 14:28:36 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s];0m \n",
      "Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  2.04it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  3.10it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  2.86it/s]\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2585051)\u001b[0;0m \u001b[1;36m(Worker_TP0 pid=2585061)\u001b[0;0m \n",
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 67/67 [00:04<00:00, 13.52it/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1074.09it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.70it/s, est. speed input: 8785.52 toks/s, output: 17.18 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1531.89it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  2.02it/s, est. speed input: 9893.97 toks/s, output: 20.57 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1306.64it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.35it/s, est. speed input: 9808.56 toks/s, output: 13.58 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1728.18it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.39it/s, est. speed input: 9790.54 toks/s, output: 14.06 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1005.83it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.03it/s, est. speed input: 9761.12 toks/s, output: 10.43 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1663.09it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.76it/s, est. speed input: 9828.80 toks/s, output: 17.85 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1579.18it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.28it/s, est. speed input: 9851.85 toks/s, output: 12.91 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1671.70it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.33it/s, est. speed input: 9662.37 toks/s, output: 13.48 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1546.00it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.18it/s, est. speed input: 9825.14 toks/s, output: 11.89 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1526.31it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.08it/s, est. speed input: 9751.91 toks/s, output: 10.88 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1801.68it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.27it/s, est. speed input: 10080.62 toks/s, output: 12.88 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1827.58it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.31it/s, est. speed input: 9773.46 toks/s, output: 13.31 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1844.46it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.61it/s, est. speed input: 10021.61 toks/s, output: 16.32 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1923.11it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.98it/s, est. speed input: 10187.94 toks/s, output: 20.15 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1707.08it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.10it/s, est. speed input: 9812.59 toks/s, output: 11.14 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1773.49it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.04it/s, est. speed input: 9840.67 toks/s, output: 10.48 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1577.40it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.33it/s, est. speed input: 10209.07 toks/s, output: 13.43 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1103.76it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.01it/s, est. speed input: 10174.56 toks/s, output: 10.15 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1600.88it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.45it/s, est. speed input: 10268.40 toks/s, output: 14.61 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1090.00it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.37it/s, est. speed input: 9889.35 toks/s, output: 13.85 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1797.82it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.12it/s, est. speed input: 10026.29 toks/s, output: 11.36 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1931.97it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.30it/s, est. speed input: 9821.48 toks/s, output: 13.19 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1364.89it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.18it/s, est. speed input: 9999.93 toks/s, output: 11.92 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1611.95it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.33it/s, est. speed input: 9742.64 toks/s, output: 13.41 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1927.53it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.28it/s, est. speed input: 9890.75 toks/s, output: 12.92 toks/s]\n",
      "25it [00:20,  1.24it/s]\n",
      "[rank2]:[W1126 14:29:17.135398320 TCPStore.cpp:125] [c10d] recvValue failed on SocketImpl(fd=213, addr=[localhost]:60322, remote=[localhost]:46401): Failed to recv, got 0 bytes. Connection was likely closed. Did the remote server shutdown or crash?\n",
      "Exception raised from recvBytes at /pytorch/torch/csrc/distributed/c10d/Utils.hpp:682 (most recent call first):\n",
      "frame #0: c10::Error::Error(c10::SourceLocation, std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >) + 0x80 (0x7f0a24688eb0 in /home/mghan/sopjt/git/venv_stackoverflow_src/lib/python3.10/site-packages/torch/lib/libc10.so)\n",
      "frame #1: <unknown function> + 0x5d694d1 (0x7f09f98b14d1 in /home/mghan/sopjt/git/venv_stackoverflow_src/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #2: <unknown function> + 0x5d6a8cd (0x7f09f98b28cd in /home/mghan/sopjt/git/venv_stackoverflow_src/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #3: <unknown function> + 0x5d6b47a (0x7f09f98b347a in /home/mghan/sopjt/git/venv_stackoverflow_src/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #4: c10d::TCPStore::check(std::vector<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> >, std::allocator<std::__cxx11::basic_string<char, std::char_traits<char>, std::allocator<char> > > > const&) + 0x31e (0x7f09f98ae19e in /home/mghan/sopjt/git/venv_stackoverflow_src/lib/python3.10/site-packages/torch/lib/libtorch_cpu.so)\n",
      "frame #5: c10d::ProcessGroupNCCL::HeartbeatMonitor::runLoop() + 0x398 (0x7f09b8d93b18 in /home/mghan/sopjt/git/venv_stackoverflow_src/lib/python3.10/site-packages/torch/lib/libtorch_cuda.so)\n",
      "frame #6: <unknown function> + 0xdc253 (0x7f0adf977253 in /lib/x86_64-linux-gnu/libstdc++.so.6)\n",
      "frame #7: <unknown function> + 0x94ac3 (0x7f0ae02f0ac3 in /lib/x86_64-linux-gnu/libc.so.6)\n",
      "frame #8: <unknown function> + 0x126850 (0x7f0ae0382850 in /lib/x86_64-linux-gnu/libc.so.6)\n",
      "\n",
      "[rank2]:[W1126 14:29:17.142489826 ProcessGroupNCCL.cpp:1783] [PG ID 0 PG GUID 0 Rank 2] Failed to check the \"should dump\" flag on TCPStore, (maybe TCPStore server has shut down too early), with error: Failed to recv, got 0 bytes. Connection was likely closed. Did the remote server shutdown or crash?\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task v|4|5|Y|20|sys_prompt10|5|0.01|ver6 실행 중: 16\n",
      "./result/sc_v_result_4_5_Y_20_sys_prompt10_5_0.01_ver6_16.csv\n",
      "/usr/share/d_ollama/data/q_output_code_y_74.csv\n",
      "VLLM\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2592513)\u001b[0;0m WARNING 11-26 14:29:18 [multiproc_worker_utils.py:273] Reducing Torch parallelism from 32 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "[W1126 14:29:21.494816907 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())\n",
      "[W1126 14:29:21.495796660 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())\n",
      "[W1126 14:29:21.504919612 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())\n",
      "[W1126 14:29:21.513499670 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2592513)\u001b[0;0m \u001b[1;36m(EngineCore_DP0 pid=2592513)\u001b[0;0m WARNING 11-26 14:29:21 [custom_all_reduce.py:144] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2592513)\u001b[0;0m \u001b[1;36m(EngineCore_DP0 pid=2592513)\u001b[0;0m WARNING 11-26 14:29:21 [custom_all_reduce.py:144] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 11-26 14:29:21 [custom_all_reduce.py:144] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 11-26 14:29:21 [custom_all_reduce.py:144] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2592513)\u001b[0;0m \u001b[1;36m(EngineCore_DP0 pid=2592513)\u001b[0;0m \u001b[1;36m(EngineCore_DP0 pid=2592513)\u001b[0;0m \u001b[1;36m(EngineCore_DP0 pid=2592513)\u001b[0;0m WARNING 11-26 14:29:21 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "WARNING 11-26 14:29:21 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "WARNING 11-26 14:29:21 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "WARNING 11-26 14:29:21 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank 2[Gloo] Rank  is connected to 3 peer ranks. 1Expected number of connected peer ranks is : 3 is connected to \n",
      "3 peer ranks. Expected number of connected peer ranks is : 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s];0m \n",
      "Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  1.98it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  3.14it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  2.87it/s]\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2592513)\u001b[0;0m \u001b[1;36m(Worker_TP0 pid=2592524)\u001b[0;0m \n",
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 67/67 [00:05<00:00, 13.36it/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1419.39it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.28it/s, est. speed input: 9089.27 toks/s, output: 12.96 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1662.43it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.97it/s, est. speed input: 9719.95 toks/s, output: 20.11 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 799.22it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.30it/s, est. speed input: 9892.98 toks/s, output: 13.17 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1804.00it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.51it/s, est. speed input: 9867.78 toks/s, output: 15.37 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1783.29it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.25it/s, est. speed input: 9651.74 toks/s, output: 12.61 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1759.36it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.74it/s, est. speed input: 9661.90 toks/s, output: 17.71 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1485.76it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.42it/s, est. speed input: 10313.52 toks/s, output: 14.38 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1824.40it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.45it/s, est. speed input: 9614.01 toks/s, output: 14.75 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1754.20it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.07it/s, est. speed input: 9644.55 toks/s, output: 10.79 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 2006.84it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.63it/s, est. speed input: 9833.14 toks/s, output: 16.60 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1761.57it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.09it/s, est. speed input: 9681.15 toks/s, output: 11.01 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 2013.59it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.96it/s, est. speed input: 9928.26 toks/s, output: 19.97 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1811.01it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.23it/s, est. speed input: 9987.40 toks/s, output: 12.48 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1903.91it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.34it/s, est. speed input: 10107.56 toks/s, output: 13.53 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1811.79it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.87it/s, est. speed input: 9940.43 toks/s, output: 19.02 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1932.86it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.59it/s, est. speed input: 9975.58 toks/s, output: 16.15 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1848.53it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.29it/s, est. speed input: 10142.43 toks/s, output: 13.03 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1596.01it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.48it/s, est. speed input: 10279.62 toks/s, output: 14.97 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1722.51it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.35it/s, est. speed input: 10010.91 toks/s, output: 13.70 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1760.09it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.26it/s, est. speed input: 10188.44 toks/s, output: 12.74 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1694.67it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.01s/it, est. speed input: 10091.59 toks/s, output: 10.00 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1795.51it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.15it/s, est. speed input: 9885.30 toks/s, output: 11.61 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1554.60it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.19s/it, est. speed input: 9776.73 toks/s, output: 8.45 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1701.54it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.29it/s, est. speed input: 9759.48 toks/s, output: 13.06 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1763.79it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.31it/s, est. speed input: 9945.44 toks/s, output: 13.21 toks/s]\n",
      "25it [00:19,  1.26it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task v|4|5|Y|20|sys_prompt10|5|0.01|ver6 실행 중: 17\n",
      "./result/sc_v_result_4_5_Y_20_sys_prompt10_5_0.01_ver6_17.csv\n",
      "/usr/share/d_ollama/data/q_output_code_y_74.csv\n",
      "VLLM\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2597422)\u001b[0;0m WARNING 11-26 14:30:03 [multiproc_worker_utils.py:273] Reducing Torch parallelism from 32 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "[W1126 14:30:06.552601342 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())\n",
      "[W1126 14:30:06.955845513 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())\n",
      "[W1126 14:30:07.255328575 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())\n",
      "[W1126 14:30:07.264628766 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : [Gloo] Rank 3\n",
      "3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank [Gloo] Rank 32 is connected to  is connected to 33 peer ranks.  peer ranks. Expected number of connected peer ranks is : Expected number of connected peer ranks is : 33\n",
      "\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2597422)\u001b[0;0m \u001b[1;36m(EngineCore_DP0 pid=2597422)\u001b[0;0m \u001b[1;36m(EngineCore_DP0 pid=2597422)\u001b[0;0m \u001b[1;36m(EngineCore_DP0 pid=2597422)\u001b[0;0m WARNING 11-26 14:30:07 [custom_all_reduce.py:144] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 11-26 14:30:07 [custom_all_reduce.py:144] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 11-26 14:30:07 [custom_all_reduce.py:144] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 11-26 14:30:07 [custom_all_reduce.py:144] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2597422)\u001b[0;0m \u001b[1;36m(EngineCore_DP0 pid=2597422)\u001b[0;0m \u001b[1;36m(EngineCore_DP0 pid=2597422)\u001b[0;0m \u001b[1;36m(EngineCore_DP0 pid=2597422)\u001b[0;0m WARNING 11-26 14:30:07 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "WARNING 11-26 14:30:07 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "WARNING 11-26 14:30:07 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "WARNING 11-26 14:30:07 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank [Gloo] Rank 21 is connected to 3 is connected to  peer ranks. Expected number of connected peer ranks is : 33 peer ranks. \n",
      "Expected number of connected peer ranks is : [Gloo] Rank 3\n",
      "3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s];0m \n",
      "Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  2.11it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  3.28it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  3.01it/s]\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2597422)\u001b[0;0m \u001b[1;36m(Worker_TP0 pid=2597432)\u001b[0;0m \n",
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 67/67 [00:05<00:00, 13.37it/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1230.36it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.05it/s, est. speed input: 9216.79 toks/s, output: 10.58 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1632.66it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.03s/it, est. speed input: 9752.94 toks/s, output: 9.82 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1681.08it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.16it/s, est. speed input: 9699.05 toks/s, output: 11.68 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1860.00it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.17it/s, est. speed input: 9910.80 toks/s, output: 11.82 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1960.87it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.31it/s, est. speed input: 9759.08 toks/s, output: 13.30 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1543.73it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.62it/s, est. speed input: 9965.04 toks/s, output: 16.46 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1675.71it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.59it/s, est. speed input: 9760.96 toks/s, output: 16.15 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1948.12it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.27it/s, est. speed input: 9837.92 toks/s, output: 12.85 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1807.89it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.49it/s, est. speed input: 9954.46 toks/s, output: 15.06 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1213.28it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.82it/s, est. speed input: 9894.19 toks/s, output: 18.55 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1672.37it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.62it/s, est. speed input: 10196.51 toks/s, output: 16.42 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1797.82it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.03it/s, est. speed input: 9548.84 toks/s, output: 10.39 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1876.65it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.51it/s, est. speed input: 9890.04 toks/s, output: 15.32 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1989.71it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.61it/s, est. speed input: 9691.91 toks/s, output: 16.31 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1911.72it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.22it/s, est. speed input: 9605.07 toks/s, output: 12.36 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1367.56it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.50it/s, est. speed input: 9954.73 toks/s, output: 15.24 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1653.25it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.47it/s, est. speed input: 9692.86 toks/s, output: 14.87 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1393.92it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.49it/s, est. speed input: 9590.28 toks/s, output: 15.11 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1718.98it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.66it/s, est. speed input: 9980.58 toks/s, output: 16.79 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1738.93it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.18it/s, est. speed input: 10019.84 toks/s, output: 11.92 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1282.27it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.46it/s, est. speed input: 9603.69 toks/s, output: 14.75 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1698.79it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.64it/s, est. speed input: 9785.99 toks/s, output: 16.70 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1579.18it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.60it/s, est. speed input: 9802.96 toks/s, output: 16.19 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1663.09it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.42it/s, est. speed input: 10013.14 toks/s, output: 14.34 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1646.12it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.48it/s, est. speed input: 10094.96 toks/s, output: 14.99 toks/s]\n",
      "25it [00:19,  1.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task v|4|5|Y|20|sys_prompt10|5|0.01|ver6 실행 중: 18\n",
      "./result/sc_v_result_4_5_Y_20_sys_prompt10_5_0.01_ver6_18.csv\n",
      "/usr/share/d_ollama/data/q_output_code_y_74.csv\n",
      "VLLM\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2602529)\u001b[0;0m WARNING 11-26 14:30:48 [multiproc_worker_utils.py:273] Reducing Torch parallelism from 32 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "[W1126 14:30:51.565385409 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())\n",
      "[W1126 14:30:51.565385331 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())\n",
      "[W1126 14:30:51.574841397 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())\n",
      "[W1126 14:30:51.575676544 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank [Gloo] Rank 1 is connected to 3 peer ranks. 3Expected number of connected peer ranks is :  is connected to 33\n",
      " peer ranks. Expected number of connected peer ranks is : 3\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2602529)\u001b[0;0m \u001b[1;36m(EngineCore_DP0 pid=2602529)\u001b[0;0m \u001b[1;36m(EngineCore_DP0 pid=2602529)\u001b[0;0m \u001b[1;36m(EngineCore_DP0 pid=2602529)\u001b[0;0m WARNING 11-26 14:30:51 [custom_all_reduce.py:144] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 11-26 14:30:51 [custom_all_reduce.py:144] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 11-26 14:30:51 [custom_all_reduce.py:144] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 11-26 14:30:51 [custom_all_reduce.py:144] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2602529)\u001b[0;0m \u001b[1;36m(EngineCore_DP0 pid=2602529)\u001b[0;0m \u001b[1;36m(EngineCore_DP0 pid=2602529)\u001b[0;0m \u001b[1;36m(EngineCore_DP0 pid=2602529)\u001b[0;0m WARNING 11-26 14:30:52 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "WARNING 11-26 14:30:52 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "WARNING 11-26 14:30:52 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "WARNING 11-26 14:30:52 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. [Gloo] Rank Expected number of connected peer ranks is : 00 is connected to \n",
      "0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank 3 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s];0m \n",
      "Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  2.02it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  3.20it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  2.93it/s]\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2602529)\u001b[0;0m \u001b[1;36m(Worker_TP0 pid=2602539)\u001b[0;0m \n",
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 67/67 [00:04<00:00, 13.41it/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 573.54it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.21it/s, est. speed input: 9359.09 toks/s, output: 12.16 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 789.89it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.22it/s, est. speed input: 9986.43 toks/s, output: 12.32 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 929.79it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.64it/s, est. speed input: 9893.12 toks/s, output: 16.65 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1794.74it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.13it/s, est. speed input: 9946.98 toks/s, output: 11.39 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1768.26it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.26it/s, est. speed input: 9934.32 toks/s, output: 12.69 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1931.97it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.53it/s, est. speed input: 9944.43 toks/s, output: 15.50 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1956.30it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.36it/s, est. speed input: 9777.54 toks/s, output: 13.71 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1677.72it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.66it/s, est. speed input: 10098.39 toks/s, output: 16.83 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1635.21it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.33it/s, est. speed input: 10166.75 toks/s, output: 13.40 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 2006.84it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.55it/s, est. speed input: 9890.24 toks/s, output: 15.69 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1679.74it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.56it/s, est. speed input: 9592.40 toks/s, output: 15.82 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1892.74it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.28it/s, est. speed input: 9890.62 toks/s, output: 12.95 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1636.48it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.13it/s, est. speed input: 10148.48 toks/s, output: 11.37 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1785.57it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.36it/s, est. speed input: 9880.26 toks/s, output: 13.78 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1555.75it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.26it/s, est. speed input: 10056.70 toks/s, output: 12.76 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1768.26it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.05it/s, est. speed input: 10002.23 toks/s, output: 10.63 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1625.70it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.34it/s, est. speed input: 10130.62 toks/s, output: 13.57 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1678.39it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.32it/s, est. speed input: 10140.75 toks/s, output: 13.34 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1869.12it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.40it/s, est. speed input: 9761.01 toks/s, output: 14.18 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1907.37it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.77it/s, est. speed input: 9594.98 toks/s, output: 18.01 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1702.23it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.01it/s, est. speed input: 9983.77 toks/s, output: 10.21 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1974.72it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.61it/s, est. speed input: 9837.34 toks/s, output: 16.37 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1776.49it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.65it/s, est. speed input: 10201.33 toks/s, output: 16.80 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1654.56it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.39it/s, est. speed input: 10366.33 toks/s, output: 14.10 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1669.71it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.36it/s, est. speed input: 10074.12 toks/s, output: 13.74 toks/s]\n",
      "25it [00:19,  1.28it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task v|4|5|Y|20|sys_prompt10|5|0.01|ver6 실행 중: 19\n",
      "./result/sc_v_result_4_5_Y_20_sys_prompt10_5_0.01_ver6_19.csv\n",
      "/usr/share/d_ollama/data/q_output_code_y_74.csv\n",
      "VLLM\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2607828)\u001b[0;0m WARNING 11-26 14:31:32 [multiproc_worker_utils.py:273] Reducing Torch parallelism from 32 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "[W1126 14:31:35.921730031 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())\n",
      "[W1126 14:31:36.441768184 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())\n",
      "[W1126 14:31:36.591637494 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())\n",
      "[W1126 14:31:36.593446778 ProcessGroupNCCL.cpp:981] Warning: TORCH_NCCL_AVOID_RECORD_STREAMS is the default now, this environment variable is thus deprecated. (function operator())\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank [Gloo] Rank 1 is connected to 33 is connected to  peer ranks. 3Expected number of connected peer ranks is :  peer ranks. 3Expected number of connected peer ranks is : 3\n",
      "\n",
      "[Gloo] Rank 2 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank [Gloo] Rank 3 is connected to 32 peer ranks.  is connected to Expected number of connected peer ranks is : 33 peer ranks. Expected number of connected peer ranks is : \n",
      "3\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2607828)\u001b[0;0m \u001b[1;36m(EngineCore_DP0 pid=2607828)\u001b[0;0m \u001b[1;36m(EngineCore_DP0 pid=2607828)\u001b[0;0m \u001b[1;36m(EngineCore_DP0 pid=2607828)\u001b[0;0m WARNING 11-26 14:31:36 [custom_all_reduce.py:144] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 11-26 14:31:36 [custom_all_reduce.py:144] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 11-26 14:31:36 [custom_all_reduce.py:144] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "WARNING 11-26 14:31:36 [custom_all_reduce.py:144] Custom allreduce is disabled because it's not supported on more than two PCIe-only GPUs. To silence this warning, specify disable_custom_all_reduce=True explicitly.\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2607828)\u001b[0;0m \u001b[1;36m(EngineCore_DP0 pid=2607828)\u001b[0;0m \u001b[1;36m(EngineCore_DP0 pid=2607828)\u001b[0;0m \u001b[1;36m(EngineCore_DP0 pid=2607828)\u001b[0;0m WARNING 11-26 14:31:37 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "WARNING 11-26 14:31:37 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "WARNING 11-26 14:31:37 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "WARNING 11-26 14:31:37 [topk_topp_sampler.py:69] FlashInfer is not available. Falling back to the PyTorch-native implementation of top-p & top-k sampling. For the best performance, please install FlashInfer.\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 0 peer ranks. Expected number of connected peer ranks is : 0\n",
      "[Gloo] Rank 0 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank 1 is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n",
      "[Gloo] Rank 3 is connected to 3[Gloo] Rank  peer ranks. Expected number of connected peer ranks is : 32\n",
      " is connected to 3 peer ranks. Expected number of connected peer ranks is : 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading safetensors checkpoint shards:   0% Completed | 0/2 [00:00<?, ?it/s];0m \n",
      "Loading safetensors checkpoint shards:  50% Completed | 1/2 [00:00<00:00,  2.02it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  3.13it/s]\n",
      "Loading safetensors checkpoint shards: 100% Completed | 2/2 [00:00<00:00,  2.88it/s]\n",
      "\u001b[1;36m(EngineCore_DP0 pid=2607828)\u001b[0;0m \u001b[1;36m(Worker_TP0 pid=2607838)\u001b[0;0m \n",
      "Capturing CUDA graphs (mixed prefill-decode, PIECEWISE): 100%|██████████| 67/67 [00:05<00:00, 13.34it/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1103.76it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.35it/s, est. speed input: 9056.26 toks/s, output: 13.64 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1452.82it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.13it/s, est. speed input: 9762.65 toks/s, output: 11.36 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 791.38it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.09it/s, est. speed input: 9850.89 toks/s, output: 10.97 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1281.49it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.12it/s, est. speed input: 9962.03 toks/s, output: 11.33 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1763.79it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.27it/s, est. speed input: 9853.83 toks/s, output: 12.85 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1669.04it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.45it/s, est. speed input: 9982.09 toks/s, output: 14.67 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1600.27it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.00s/it, est. speed input: 10081.86 toks/s, output: 10.08 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1796.28it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.45it/s, est. speed input: 10311.95 toks/s, output: 14.74 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1797.05it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.58it/s, est. speed input: 10066.75 toks/s, output: 15.96 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1489.98it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:01<00:00,  1.22s/it, est. speed input: 9839.44 toks/s, output: 8.25 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1971.01it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.57it/s, est. speed input: 9830.77 toks/s, output: 15.94 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1880.85it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.20it/s, est. speed input: 9959.53 toks/s, output: 12.11 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1804.78it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.56it/s, est. speed input: 10193.86 toks/s, output: 15.85 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1959.04it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.88it/s, est. speed input: 10305.86 toks/s, output: 19.06 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1886.78it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.27it/s, est. speed input: 10083.97 toks/s, output: 12.83 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1588.75it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.71it/s, est. speed input: 9852.00 toks/s, output: 17.28 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1878.33it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.27it/s, est. speed input: 10160.25 toks/s, output: 12.76 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1787.85it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.51it/s, est. speed input: 9901.51 toks/s, output: 15.27 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1661.11it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.56it/s, est. speed input: 9959.44 toks/s, output: 15.76 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1788.62it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.28it/s, est. speed input: 9934.57 toks/s, output: 12.90 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1705.69it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.02it/s, est. speed input: 9862.65 toks/s, output: 10.32 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1689.21it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.35it/s, est. speed input: 10326.69 toks/s, output: 13.66 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1862.48it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.34it/s, est. speed input: 10038.43 toks/s, output: 13.58 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 1734.62it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.16it/s, est. speed input: 10423.31 toks/s, output: 11.75 toks/s]\n",
      "Adding requests: 100%|██████████| 1/1 [00:00<00:00, 946.37it/s]\n",
      "Processed prompts: 100%|██████████| 1/1 [00:00<00:00,  1.82it/s, est. speed input: 10235.78 toks/s, output: 18.50 toks/s]\n",
      "25it [00:20,  1.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task v_4_5_Y_sys_prompt10_5 완료\n",
      "모든 작업 완료\n"
     ]
    }
   ],
   "source": [
    "from multiprocessing import Process\n",
    "    \n",
    "def task(llm_model, few_shot_n, test_n, q_src_yn, ver, p_ver, sc_num, temperature, excel_ver):\n",
    "\n",
    "    print(f\"Task {llm_model}_{few_shot_n}_{test_n}_{q_src_yn}_{p_ver}_{sc_num} 시작\")\n",
    "    for i in range(ver):\n",
    "        print(f\"Task {llm_model}|{few_shot_n}|{test_n}|{q_src_yn}|{ver}|{p_ver}|{sc_num}|{temperature}|{excel_ver} 실행 중: {i}\")\n",
    "\n",
    "        sc.Self_Consistency(   llm_model\n",
    "                            , few_shot_n\n",
    "                            , test_n\n",
    "                            , q_src_yn\n",
    "                            , ver\n",
    "                            , p_ver\n",
    "                            , sc_num\n",
    "                            , temperature\n",
    "                            , excel_ver\n",
    "                            , i)\n",
    "    \n",
    "    print(f\"Task {llm_model}_{few_shot_n}_{test_n}_{q_src_yn}_{p_ver}_{sc_num} 완료\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "\n",
    "    process3 = Process(target=task, args=('v', 4, 5, 'Y', 20, 'sys_prompt10', 5, 0.01, 'ver6'))\n",
    "    process3.start()\n",
    "    \n",
    "    process3.join()\n",
    "    print(\"모든 작업 완료\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = './result'\n",
    "file_list = os.listdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sc_calc_acc_condition_with_temp_with_sc(llm_model, few_shot_n, test_n, q_src_yn, ver, p_ver, sc_num, temp, excel_ver):\n",
    "    tmp = pd.DataFrame()\n",
    "    df_eval = pd.DataFrame()\n",
    "    acc_list = []\n",
    "    path = './result'\n",
    "    # ./result/sc_l_result_4_15_Y_30_sys_prompt8_0.01_ver1_0.csv\n",
    "    file_list = os.listdir(path)\n",
    "    opt_file = [x for x in file_list if x.startswith(f'sc_{llm_model}_result_{few_shot_n}_{test_n}_{q_src_yn}_{ver}_{p_ver}_{sc_num}_{temp}_{excel_ver}')]\n",
    "    opt_file = [x for x in opt_file if x.endswith(f'.csv')]\n",
    "\n",
    "    df = pd.DataFrame()\n",
    "\n",
    "    \n",
    "    if len(opt_file)>0 : \n",
    "        for f in opt_file:\n",
    "            tmp = pd.read_csv(f'{path}/{f}', index_col =0)\n",
    "            tmp = tmp.dropna()\n",
    "\n",
    "            tmp['gold'] = tmp['answer_encode'].apply(lambda x : re.sub(r'[^012]', '', x))\n",
    "            tmp['o_result'] = tmp['result'].apply(lambda x : re.sub(r'[^012]', '', x))\n",
    "            tmp = tmp[tmp['o_result'].isin(['1', '0', '2'])]\n",
    "\n",
    "            \n",
    "            gold_df = tmp[['id', 'gold']].drop_duplicates()\n",
    "            chk_cnt = tmp.groupby(['id', 'o_result']).count().reset_index()[['id', 'o_result', 'question']]\n",
    "            chk_cnt = chk_cnt.rename(columns = {'question': 'cnt'})\n",
    "            chk_cnt = chk_cnt[chk_cnt['cnt'] == sc_num]\n",
    "            chk_cnt = chk_cnt.sort_values(by = ['id', 'cnt'], ascending=[True, False]).groupby(['id']).head(1)\n",
    "            df_eval = pd.merge(gold_df, chk_cnt, on = ['id'])\n",
    "\n",
    "            df_eval['equal_yn'] = np.where(df_eval['gold']==df_eval['o_result'], 1, 0)\n",
    "            acc = (df_eval['equal_yn'].sum()/df_eval.shape[0])*100  \n",
    "            acc_list.append(acc)\n",
    "            df = pd.concat([df, df_eval], axis =0)\n",
    "            \n",
    "        df['equal_yn'] = np.where(df['gold']==df['o_result'], 1, 0)\n",
    "        y_true = df['o_result']\n",
    "        y_pred = df['gold']\n",
    "        print(metrics.classification_report(y_true, y_pred, digits=3))\n",
    "        \n",
    "        acc = (df['equal_yn'].sum()/df.shape[0])*100            \n",
    "        print(f'{llm_model}_result_{few_shot_n}_{test_n}_{q_src_yn} : ', acc)\n",
    "        return acc_list\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process(target=task, args=('l', 1, 10, 'Y', 10, 'sys_prompt10', 3, 0.01, 'ver5'))\n",
    "# Process(target=task, args=('l', 2, 10, 'Y', 10, 'sys_prompt10', 3, 0.01, 'ver5'))\n",
    "# Process(target=task, args=('l', 4, 10, 'Y', 10, 'sys_prompt10', 5, 0.01, 'ver5'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# process3 = Process(target=task, args=                 ('v', 4, 5, 'Y', 1, 'sys_prompt10', 5, 0.01, 'ver6'))\n",
    "list_ =         sc_calc_acc_condition_with_temp_with_sc('v', 4, 5, 'Y', 1, 'sys_prompt10', 5,  0.01, 'ver6')\n",
    "print(list_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 100개 sample에 대한 난이도 측정 후에 거기서 basic 혹은 advanced에 해당하는것을 뽑아서 sample로 전달하기......"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lib.annotation.D_Annotation as da\n",
    "import lib.annotation.Self_Consistency as sc\n",
    "import lib.annotation.Sample_Insert as si\n",
    "import lib.annotation.Q_Extract as qe\n",
    "import lib.annotation.SampleSelf_Consistency as ssc\n",
    "\n",
    "\n",
    "q_output = pd.read_csv(f'./4th_snapshop2_sample.csv')\n",
    "    \n",
    "print(f\"SampleSelf_Consistency start\")\n",
    "sample_sc = ssc.SampleSelf_Consistency(q_output) \n",
    "print(f\"SampleSelf_Consistency end\")\n",
    "\n",
    "print(f\"write_promt start\")\n",
    "chk_list = sample_sc.write_promt()\n",
    "print(f\"write_promt end\")\n",
    "\n",
    "print(f\"calc_acc_for_v start\")\n",
    "result_df = sample_sc.calc_acc_for_v()\n",
    "print(f\"calc_acc_for_v end\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_stackoverflow_src",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
